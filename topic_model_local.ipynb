{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luisanovaes/ufam_project/blob/main/topic_model_local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQGA2kdK45Vc"
      },
      "outputs": [],
      "source": [
        "#Check GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topification"
      ],
      "metadata": {
        "id": "sk5hJolI4F89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bzv0KGN9o_s"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic==0.12.0\n",
        "!pip install pyspark\n",
        "!pip install keyphrase-vectorizers\n",
        "!pip install top2vec\n",
        "!pip install sentence-transformers\n",
        "!pip install gensim==4.2.0\n",
        "!pip install joblib==1.1.0\n",
        "!pip install --upgrade pip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "print(gensim.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE6wnJICgoao",
        "outputId": "29b9e352-fc00-427a-d4a0-e52c11ceb09e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9FxFu4diPfp"
      },
      "source": [
        "## Import JusBERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwKB5rzZiYml"
      },
      "outputs": [],
      "source": [
        "from numpy.lib.npyio import save\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "\n",
        "class jusBERTopic:\n",
        "    def __init__(self, seed=2021):\n",
        "        self.model = None\n",
        "        self.seed = seed\n",
        "        self.top_n_words = 20\n",
        "        nltk.download('punkt')\n",
        "\n",
        "        import torch\n",
        "        torch.manual_seed(self.seed)\n",
        "        torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def train_doc2vec(self, documents):\n",
        "        min_count = 50\n",
        "        hs = 1\n",
        "        negative = 0\n",
        "        epochs = 40\n",
        "        doc2vec_args = {\"vector_size\": 300,\n",
        "                        \"min_count\": min_count,\n",
        "                        \"window\": 15,\n",
        "                        \"sample\": 1e-5,\n",
        "                        \"negative\": negative,\n",
        "                        \"hs\": hs,\n",
        "                        \"epochs\": epochs,\n",
        "                        \"dm\": 0,\n",
        "                        \"dbow_words\": 1}\n",
        "\n",
        "        tokenized_doc = []\n",
        "        for d in documents:\n",
        "            tokenized_doc.append(word_tokenize(d, language='english'))\n",
        "        # Convert tokenized document into gensim formated tagged data\n",
        "        tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]\n",
        "        model = Doc2Vec(**doc2vec_args)\n",
        "        model.build_vocab(tagged_data)\n",
        "        model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "        return model\n",
        "\n",
        "    def doc2vec_infer(corpus, model_path):\n",
        "      # load doc2vec model\n",
        "      d2v_model = Doc2Vec.load(model_path)\n",
        "      embedding_model = d2v_model.wv\n",
        "      # tokenize documents\n",
        "      tokenized_doc = [word_tokenize(d, language='english') for d in corpus]\n",
        "      doc_infer = d2v_model.infer_vector(tokenized_doc[0], steps=40)\n",
        "      embedding = [doc_infer]\n",
        "      for document in tokenized_doc[1:]:\n",
        "          doc_infer = d2v_model.infer_vector(document, steps=40)\n",
        "          embedding = np.append(embedding, [np.array(doc_infer)], axis=0)\n",
        "      return embedding_model, embedding        \n",
        "\n",
        "    def build_model(self, option, documents, embed_documents, lm_path, calculate_probabilities=False): \n",
        "        if option == 'sentence':  # sentence transformer\n",
        "            emb_model = SentenceTransformer(lm_path)\n",
        "            embedding_model = emb_model\n",
        "            embedding = emb_model.encode(embed_documents, show_progress_bar=True)\n",
        "        elif option == 'doc2vec':\n",
        "            print('Executing bertopic + doc2vec')\n",
        "            try:\n",
        "            # load pre-trained doc2vec model\n",
        "              model = np.load('content/drive/MyDrive/Colab-Notebooks/luisa-novaes/embedding-vector.npy')\n",
        "            except:\n",
        "              model = self.train_doc2vec(documents=documents)\n",
        "              embedding_model = model.wv\n",
        "              embedding = model.dv.vectors\n",
        "            #np.save('/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/embedding-vector', embedding)\n",
        "            #print(embedding)\n",
        "        else:  # default\n",
        "            embedding_model = None\n",
        "            embedding = None\n",
        "\n",
        "        print('Done creating embeddings.')\n",
        "\n",
        "        self.top_n_words = 20\n",
        "        # initialize model\n",
        "        self.model = BERTopic(language='english',\n",
        "                              top_n_words=self.top_n_words,\n",
        "                              calculate_probabilities=calculate_probabilities,\n",
        "                              embedding_model=embedding_model,\n",
        "                              n_gram_range=(1, 2),\n",
        "                              verbose=True,\n",
        "                              diversity=0.1)\n",
        "        print('Done initializing bertopic.')\n",
        "\n",
        "        # fit model to documents\n",
        "        topics, probabilities = self.model.fit_transform(documents=documents, embeddings=embedding)\n",
        "\n",
        "        print(self.model.get_topic_freq())\n",
        "        return self.model, topics, probabilities\n",
        "\n",
        "    def get_word_distribution_per_topic(self):\n",
        "        topics = self.model.get_topics()\n",
        "        all_topics = {}\n",
        "        for topic_id in topics.keys():\n",
        "            words = []\n",
        "            for item in topics[topic_id]:\n",
        "                words.append(item[0])\n",
        "            all_topics[topic_id] = words\n",
        "        return all_topics\n",
        "\n",
        "    def get_num_topics(self):\n",
        "        return len(self.model.get_topics())\n",
        "\n",
        "    def print_topics(self):\n",
        "        list_of_words = []\n",
        "        topics = self.model.get_topics()\n",
        "        all_topics = {}\n",
        "        for topic_id in topics.keys():\n",
        "            words = []\n",
        "            for item in topics[topic_id]:\n",
        "                words.append(item[0])\n",
        "            if topic_id != -1:\n",
        "                new_words = [x for x in words if x != '']\n",
        "                list_of_words.append(new_words)\n",
        "            print(f\"Topic %d: {words}\" % topic_id)\n",
        "        return list_of_words\n",
        "\n",
        "    def stats_topic_per_document(self, all_topics, contribution, corpus, doc_ids_list):\n",
        "        # Init output\n",
        "        topic_csv_df = pd.DataFrame()\n",
        "        doc_csv_df = pd.DataFrame()\n",
        "\n",
        "        words_per_topic = self.get_word_distribution_per_topic()\n",
        "\n",
        "        topic_num = pd.Series(all_topics)\n",
        "        topic_num.astype(int)\n",
        "        max_score = pd.Series(contribution)\n",
        "        max_score = np.round(max_score, 4)\n",
        "        doc_csv_df = pd.concat([topic_num, max_score], axis=1)\n",
        "        doc_csv_df.columns = ['Dominant_Topic', 'Perc_Contribution']\n",
        "\n",
        "        # adding document id and numero unificado\n",
        "        juris_ids = pd.Series(doc_ids_list)\n",
        "        doc_csv_df = pd.concat([juris_ids, doc_csv_df], axis=1)\n",
        "\n",
        "        # Add original text to the end of the output\n",
        "        contents = pd.Series(corpus)\n",
        "        doc_csv_df = pd.concat([doc_csv_df, contents], axis=1)\n",
        "\n",
        "        # Format\n",
        "        doc_csv_df.columns = ['Document_id', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Document']\n",
        "\n",
        "        # ignoring the topic -1\n",
        "        for topic_num in range(0, len(words_per_topic)-1):\n",
        "            wp = words_per_topic[topic_num]\n",
        "            topic_keywords = \", \".join([word for word in wp])\n",
        "            topic_csv_df = topic_csv_df.append(pd.Series([topic_num, topic_keywords]), ignore_index=True)\n",
        "        topic_csv_df.columns = ['Topic', 'Keywords']\n",
        "\n",
        "        return doc_csv_df, topic_csv_df\n",
        "\n",
        "    def stats_topic_per_document_with_probabilities(self, contribution, corpus, doc_ids_list):\n",
        "        # Init output\n",
        "        topic_csv_df = pd.DataFrame()\n",
        "        doc_csv_df = pd.DataFrame()\n",
        "\n",
        "        words_per_topic = self.get_word_distribution_per_topic()\n",
        "\n",
        "        # Get main topic in each document\n",
        "        for i, row in enumerate(contribution):\n",
        "            # For now I am ignoring this feature of BERTopic, that is to return topic = -1  for outliers\n",
        "            # (documents that do not have a topic assigned)\n",
        "            max_score = np.max(row)\n",
        "            topic_num = np.argmax(row)\n",
        "\n",
        "            #ind = np.argpartition(row, -20)[-20:]\n",
        "            #top5 = ind[np.argsort(row[ind])]\n",
        "            #top5 = np.flip(top5)\n",
        "            top5 = np.flip(np.argsort(row))\n",
        "            doc_csv_df = doc_csv_df.append(pd.Series([int(topic_num), round(max_score, 4), top5]), ignore_index=True)\n",
        "        doc_csv_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Top5_Topics']\n",
        "\n",
        "        # adding document id and numero unificado\n",
        "        juris_ids = pd.Series(doc_ids_list)\n",
        "        doc_csv_df = pd.concat([juris_ids, doc_csv_df], axis=1)\n",
        "\n",
        "        # Add original text to the end of the output\n",
        "        contents = pd.Series(corpus)\n",
        "        doc_csv_df = pd.concat([doc_csv_df, contents], axis=1)\n",
        "\n",
        "        # Format\n",
        "        #doc_csv_df = doc_csv_df.reset_index()\n",
        "        doc_csv_df.columns = ['Document_id', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Top5_Topics', 'Ementa']\n",
        "\n",
        "        # ignoring the topic -1\n",
        "        for topic_num in range(0, len(words_per_topic)-1):\n",
        "            wp = words_per_topic[topic_num]\n",
        "            topic_keywords = \", \".join([word for word in wp])\n",
        "            topic_csv_df = topic_csv_df.append(pd.Series([topic_num, topic_keywords]), ignore_index=True)\n",
        "        topic_csv_df.columns = ['Topic', 'Keywords']\n",
        "\n",
        "        return doc_csv_df, topic_csv_df\n",
        "\n",
        "    def array_to_string(self, my_list):\n",
        "        return \", \".join([word for word in my_list])\n",
        "\n",
        "    def stats_topic_per_document_spark(self, spark, all_topics, contribution, corpus, doc_ids_list):\n",
        "        contribution = [float(np.round(i, 4)) for i in contribution]\n",
        "        dfSchema = StructType([\n",
        "            StructField(\"Document_id\", StringType(), True),\n",
        "            StructField(\"Dominant_Topic\", IntegerType(), True),\n",
        "            StructField(\"Topic_Perc_Contrib\", DoubleType(), True),\n",
        "            StructField(\"Document\", StringType(), True)\n",
        "        ])\n",
        "        data = zip(doc_ids_list, all_topics, contribution, corpus)\n",
        "        columns = ['Document_id', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Document']\n",
        "        doc_csv_df = spark.createDataFrame(data, columns, dfSchema)\n",
        "        doc_csv_df.withColumn(\"Dominant_Topic\", col(\"Dominant_Topic\").cast(\"int\"))\n",
        "\n",
        "        words_per_topic = self.get_word_distribution_per_topic()\n",
        "        array_to_string_udf = udf(self.array_to_string, StringType())\n",
        "        dfTopicsSchema = StructType([\n",
        "            StructField(\"Topic\", StringType(), True),\n",
        "            StructField(\"Keywords\", StringType(), True)\n",
        "        ])\n",
        "        columns = ['Topic', 'Keywords']\n",
        "        data = zip(words_per_topic.keys(), words_per_topic.values())\n",
        "        topic_csv_df = spark.createDataFrame(data, columns, dfTopicsSchema)\n",
        "        topic_csv_df = topic_csv_df.withColumn('Keywords', array_to_string_udf(topic_csv_df[\"Keywords\"]))\n",
        "        return doc_csv_df, topic_csv_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaB1UUXRW70P"
      },
      "outputs": [],
      "source": [
        "#Função indicando o caminho dos arquivos .py que chamaremos embaixo\n",
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/tm_geral-main')\n",
        "\n",
        "from glob import glob\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#JusTop2Vec\n",
        "\n",
        "# https://github.com/ddangelov/Top2Vec\n",
        "\n",
        "from top2vec import Top2Vec\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "\n",
        "class jusTop2Vec:\n",
        "    def __init__(self, language_model='universal-sentence-encoder-multilingual', seed=2021):\n",
        "        self.LM = language_model\n",
        "        self.seed = seed\n",
        "\n",
        "        import torch\n",
        "        torch.manual_seed(self.seed)\n",
        "        torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def train(self, documents):\n",
        "        return Top2Vec(documents, embedding_model=self.LM)\n",
        "\n",
        "    def get_num_topics(self, model):\n",
        "        return model.get_num_topics()\n",
        "\n",
        "    def print_topics(self, model, num_topics=None):\n",
        "        \"\"\"\n",
        "        Print word distribution per topic\n",
        "\n",
        "        :parameters:\n",
        "            num_topics: number of topics\n",
        "        \"\"\"\n",
        "        topics_words, word_scores, topic_nums = model.get_topics(num_topics)\n",
        "\n",
        "        for topic in topic_nums:\n",
        "            print(f\"Topic %d: {topics_words[topic]}\" % topic)\n",
        "\n",
        "    def get_word_distribution_per_topic(self, model, num_topics=None):\n",
        "        \"\"\"\n",
        "        Get distribution of words per topic\n",
        "\n",
        "        :parameters:\n",
        "            num_topics: number of topics\n",
        "\n",
        "        :return:\n",
        "           distribution of words per topics\n",
        "        \"\"\"\n",
        "        topics_words, word_scores, topic_nums = model.get_topics(num_topics)\n",
        "        all_topics = {}\n",
        "        for topic in topic_nums:\n",
        "            all_topics[topic] = topics_words[topic]\n",
        "        return all_topics\n",
        "\n",
        "    def get_topics_per_doc(self, model, total_num_docs):\n",
        "        \"\"\"\n",
        "        Get distribution of topics per document\n",
        "\n",
        "        :return:\n",
        "           topic per document\n",
        "        \"\"\"\n",
        "        topic_per_document = []\n",
        "        for doc_id in range(0, total_num_docs):\n",
        "            doc_topics, doc_dist, topic_words, topic_word_scores = model.get_documents_topics([doc_id])\n",
        "            topic_per_document.append((doc_topics[0], doc_dist[0]))\n",
        "        return topic_per_document\n",
        "\n",
        "    def show_wordcloud(self, model, topic_id):\n",
        "        model.generate_topic_wordcloud(topic_id)\n",
        "\n",
        "    def search_documents_by_topic(self, model, topic_num=0, num_docs=10):\n",
        "        return model.search_documents_by_topic(topic_num, num_docs)\n",
        "\n",
        "    def get_keywords_per_topic(self, model, num_topics=1, top_n=10):\n",
        "        keywords = {}\n",
        "        words_per_topic = self.get_word_distribution_per_topic(model, num_topics=num_topics)\n",
        "        for k in range(0, num_topics):\n",
        "            wp = words_per_topic[k]\n",
        "            keywords[k] = [word for word in wp[0:top_n]]\n",
        "        return keywords\n",
        "\n",
        "    def stats_topic_per_document(self, model, corpus, doc_ids_list, num_topics=1, top_n_words=10):\n",
        "        # Init output\n",
        "        topic_csv_df = pd.DataFrame()\n",
        "        doc_csv_df = pd.DataFrame()\n",
        "\n",
        "        topics_per_doc = self.get_topics_per_doc(model, len(corpus))\n",
        "        words_per_topic = self.get_word_distribution_per_topic(model, num_topics=num_topics)\n",
        "\n",
        "        # Get main topic in each document\n",
        "        for i, row in enumerate(topics_per_doc):\n",
        "            topic_num = row[0]\n",
        "            max_score = row[1]\n",
        "            doc_csv_df = doc_csv_df.append(pd.Series([int(topic_num), round(max_score, 4)]), ignore_index=True)\n",
        "        doc_csv_df.columns = ['Dominant_Topic', 'Perc_Contribution']\n",
        "\n",
        "        # adding document id and numero unificado\n",
        "        juris_ids = pd.Series(doc_ids_list)\n",
        "        doc_csv_df = pd.concat([juris_ids, doc_csv_df], axis=1)\n",
        "\n",
        "        # Add original text to the end of the output\n",
        "        contents = pd.Series(corpus)\n",
        "        doc_csv_df = pd.concat([doc_csv_df, contents], axis=1)\n",
        "\n",
        "        # Format\n",
        "        doc_csv_df.columns = ['Document_id', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Document']\n",
        "\n",
        "        for topic_num in range(0, len(words_per_topic)):\n",
        "            wp = words_per_topic[topic_num]\n",
        "            topic_keywords = \", \".join([word for word in wp[0:top_n_words]])\n",
        "            topic_csv_df = topic_csv_df.append(pd.Series([topic_num, topic_keywords]), ignore_index=True)\n",
        "        topic_csv_df.columns = ['Topic', 'Keywords']\n",
        "\n",
        "        return doc_csv_df, topic_csv_df\n",
        "\n",
        "    def array_to_string(self, my_list, top_n_words):\n",
        "        print(my_list)\n",
        "        return \", \".join([word for word in my_list[0:top_n_words]])\n",
        "\n",
        "    def stats_topic_per_document_spark(self, spark, model, corpus, doc_ids_list, num_topics=1, top_n_words=10):\n",
        "        topics_per_doc = self.get_topics_per_doc(model, len(corpus))\n",
        "        words_per_topic = self.get_word_distribution_per_topic(model, num_topics=num_topics)\n",
        "        list_topics_per_doc = list(map(list, zip(*topics_per_doc)))\n",
        "        list_topics_per_doc[0] = [int(i) for i in list_topics_per_doc[0]]\n",
        "        list_topics_per_doc[1] = [float(i) for i in list_topics_per_doc[1]]\n",
        "        dfSchema = StructType([\n",
        "            StructField(\"Document_id\", StringType(), True),\n",
        "            StructField(\"Dominant_Topic\", IntegerType(), True),\n",
        "            StructField(\"Topic_Perc_Contrib\", DoubleType(), True),\n",
        "            StructField(\"Document\", StringType(), True)\n",
        "        ])\n",
        "        data = zip(doc_ids_list, list_topics_per_doc[0], list_topics_per_doc[1], corpus)\n",
        "        columns = ['Document_id', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Document']\n",
        "        doc_csv_df = spark.createDataFrame(data, columns, dfSchema)\n",
        "        doc_csv_df.withColumn(\"Dominant_Topic\", col(\"Dominant_Topic\").cast(\"int\"))\n",
        "\n",
        "        topics = list(map(int, words_per_topic.keys()))\n",
        "        list_of_words = [self.array_to_string(words, top_n_words) for words in words_per_topic.values()]\n",
        "        dfTopicsSchema = StructType([\n",
        "            StructField(\"Topic\", StringType(), True),\n",
        "            StructField(\"Keywords\", StringType(), True)\n",
        "        ])\n",
        "        columns = ['Topic', 'Keywords']\n",
        "        data = zip(topics, list_of_words)\n",
        "        topic_csv_df = spark.createDataFrame(data, columns, dfTopicsSchema)\n",
        "        topic_csv_df = topic_csv_df.withColumn('Keywords', topic_csv_df[\"Keywords\"])\n",
        "\n",
        "        return doc_csv_df, topic_csv_df\n"
      ],
      "metadata": {
        "id": "LRTqYMMOUDOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-9KKV0VWYKp"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#from jusTop2Vec import jusTop2Vec\n",
        "#from jusBERTopic import jusBERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "\n",
        "class LoadData:\n",
        "    def load_data(self):\n",
        "        documents = []\n",
        "        list_of_ids = []\n",
        "        i=0\n",
        "      \n",
        "        mypath = '/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/Coliee_2021'\n",
        "        for txt in glob(os.path.join(mypath, '*.txt')):\n",
        "          with open(txt) as tfile:\n",
        "            text = tfile.read()         \n",
        "            text = text.replace('\\n',' ').replace('FRAGMENT_SUPPRESSED','')\n",
        "            documents.append(text)\n",
        "            list_of_ids.append(txt)\n",
        "            list_of_ids[:] = [d.replace('/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/Coliee_2021/','') for d in list_of_ids]\n",
        "     \n",
        "        return documents, list_of_ids\n",
        "\n",
        "# prepare data to run topic modeling solution\n",
        "class PreprocessingData:\n",
        "    def __init__(self, min_df=1, max_df=0.75, token_pattern=r\"[a-zA-Z]\\w*\"): ##Mexer nisso?\n",
        "        self.min_df = min_df\n",
        "        self.max_df = max_df\n",
        "        self.token_pattern = token_pattern\n",
        "        # list of stop words in english\n",
        "        nltk.download('stopwords')\n",
        "        self.en_stopwords = nltk.corpus.stopwords.words('english') # Mudado \n",
        "        self.en_stopwords.extend(['refd'])  # Add 'refd' as stopword \n",
        "\n",
        "\n",
        "    def remove_punctuation(self, text):\n",
        "        \"\"\"\n",
        "        Remove punctuations\n",
        "\n",
        "        :parameters:\n",
        "            text: sentence (string)\n",
        "\n",
        "        :return: preprocessed documents without punctuations\n",
        "        \"\"\"\n",
        "        punctuation = r'[/.!$%^&#*+\\'\\\"()-.,:;<=>?@[\\]{}|]'\n",
        "        return re.sub(punctuation, '', text)\n",
        "\n",
        "    def strip_accents_from_word(self, text):\n",
        "        \"\"\"\n",
        "        Strip accents from input String.\n",
        "\n",
        "        :parameters:\n",
        "            text: word (string)\n",
        "\n",
        "        :return: preprocessed word without accent\n",
        "         \"\"\"\n",
        "        text = unicodedata.normalize('NFD', text)\n",
        "        text = text.encode('ascii', 'ignore')\n",
        "        text = text.decode(\"utf-8\")\n",
        "        return str(text)\n",
        "\n",
        "    def strip_accents(self, text):\n",
        "        \"\"\"\n",
        "        Strip accents from text.\n",
        "\n",
        "        :parameters:\n",
        "            text: sentence (string)\n",
        "\n",
        "        :return: preprocessed documents without accent\n",
        "         \"\"\"\n",
        "        return \" \".join([self.strip_accents_from_word(word) for word in text.split()])\n",
        "\n",
        "    def remove_URL(self, text):\n",
        "        \"\"\"\n",
        "        Remove URLs from text\n",
        "        :return: text without URLs\n",
        "        \"\"\"\n",
        "        return re.sub(r\"<.*?>\", \" \", text)\n",
        "\n",
        "    def clean_text(self, documents):\n",
        "        \"\"\"\n",
        "        Clean (preprocess) text data: parse URL, remove punctuation, convert to lowercase, remove stopword, and\n",
        "        strip accents\n",
        "        :param documents: list of documents\n",
        "        :return: preprocessed_text: list of preprocessed documents\n",
        "        \"\"\"\n",
        "        # preprocess jusbrasil URLs\n",
        "        preprocessed_text = []\n",
        "        for text in documents:\n",
        "            preprocessed_text.append(self.remove_URL(text))\n",
        "        # We have to decide whether we want to remove or keep punctuation\n",
        "        # agravodeinstrumentoai70068698984rs or lei_de_criacao_do_pis_lei_complementar_7_70\n",
        "        preprocessed_text = [self.remove_punctuation(doc) for doc in preprocessed_text]\n",
        "        # convert to lowercase\n",
        "        preprocessed_text = [doc.lower() for doc in preprocessed_text]\n",
        "        # remove stopwords\n",
        "        preprocessed_text = [' '.join([w for w in doc.split() if len(w) > 1 and w not in self.en_stopwords])\n",
        "                             for doc in preprocessed_text]\n",
        "        # strip accents\n",
        "        # preprocessed_text = [self.strip_accents(doc) for doc in preprocessed_text]\n",
        "        return preprocessed_text\n",
        "\n",
        "    def prepare_text_for_topicmodel(self, documents, list_doc_ids):\n",
        "        \"\"\"\n",
        "        Prepare data for BERTopic topic discovery\n",
        "        :param documents: list of documents\n",
        "        :param list_doc_ids: list of documents' ids\n",
        "        :return: preprocessed_docs: list of preprocessed documents\n",
        "                 ids_docs_removed: list of ids to be removed\n",
        "        \"\"\"\n",
        "        # clean the data: parser URL, lowercase, remove punctuation, and remove stopwords\n",
        "        preprocessed_docs_tmp = self.clean_text(documents)\n",
        "\n",
        "        # stop_word = None. Stopwords were removed in clean_text()\n",
        "        vectorizer = CountVectorizer(stop_words=None,\n",
        "                                     tokenizer=None,\n",
        "                                     min_df=self.min_df,\n",
        "                                     max_df=self.max_df,\n",
        "                                     ngram_range=(1, 2),\n",
        "                                     token_pattern=self.token_pattern,\n",
        "                                     lowercase=True)\n",
        "\n",
        "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
        "        vocabulary = set(vectorizer.get_feature_names())\n",
        "\n",
        "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w.lower() in vocabulary])\n",
        "                                 for doc in preprocessed_docs_tmp]\n",
        "\n",
        "        list_ids_removed = []\n",
        "        preprocessed_docs, unpreprocessed_docs = [], []\n",
        "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
        "            if len(doc) > 0:\n",
        "                preprocessed_docs.append(doc)\n",
        "                unpreprocessed_docs.append(documents[i])\n",
        "            else:\n",
        "                list_ids_removed.append(list_doc_ids[i])\n",
        "\n",
        "        return preprocessed_docs, list_ids_removed\n",
        "\n",
        "\n",
        "class TopicModel:\n",
        "    def __init__(self, corpus=[], embed_corpus=[], doc_ids_list=[], embedding_option='doc2vec', embedding_path=''):\n",
        "        self.corpus = corpus\n",
        "        if embed_corpus:\n",
        "            self.embed_corpus = embed_corpus\n",
        "        else:\n",
        "            self.embed_corpus = corpus\n",
        "        self.doc_ids_list = doc_ids_list\n",
        "        self.seed = 2021\n",
        "        self.embedding_option = embedding_option\n",
        "        self.embedding_path = embedding_path\n",
        "\n",
        "    def run_bertopic(self, calculate_probabilities=False):\n",
        "        \"\"\"\n",
        "        Run BERTopic and save description of topics and statistics of topics per document into csv files.\n",
        "        :param calculate_probabilities: whether to calculate the probabilities of all topics per document instead of\n",
        "        the probability of the assigned topic per document.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        bert_topic = jusBERTopic(seed=self.seed)\n",
        "        model, topics, probabilities = bert_topic.build_model(option=self.embedding_option,\n",
        "                                                              documents=self.corpus,\n",
        "                                                              embed_documents=self.embed_corpus,\n",
        "                                                              lm_path=self.embedding_path,\n",
        "                                                              calculate_probabilities=calculate_probabilities)\n",
        "        print(\"entrou no bert-topic\")\n",
        "        model.save('/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/saved-model')\n",
        "\n",
        "        if calculate_probabilities:\n",
        "            # not optimized for spark/parquet yet\n",
        "            doc_csv_df, topic_csv_df = bert_topic.stats_topic_per_document_with_probabilities(probabilities,\n",
        "                                                                                              self.corpus,\n",
        "                                                                                              self.doc_ids_list)\n",
        "\n",
        "        else:\n",
        "            doc_csv_df, topic_csv_df = bert_topic.stats_topic_per_document(topics,\n",
        "                                                                           probabilities,\n",
        "                                                                           self.corpus,\n",
        "                                                                           self.doc_ids_list)\n",
        "\n",
        "        doc_csv_df.to_csv('/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/bertopic_documents.csv')\n",
        "        topic_csv_df.to_csv('/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/bertopic_topics.csv')\n",
        "\n",
        "    def run_top2vec(self):\n",
        "        \"\"\"\n",
        "        Run Top2Vec and save description of topics and statistics of topics per document into csv files.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        t2v = jusTop2Vec(language_model=\"doc2vec\", seed=self.seed)\n",
        "        model = t2v.train(self.corpus)\n",
        "\n",
        "        num_topics = t2v.get_num_topics(model)\n",
        "        t2v.print_topics(model)\n",
        "\n",
        "        doc_csv_df, topic_csv_df = t2v.stats_topic_per_document(model,\n",
        "                                                                self.corpus,\n",
        "                                                                self.doc_ids_list,\n",
        "                                                                num_topics=num_topics,\n",
        "                                                                top_n_words=20)\n",
        "\n",
        "        doc_csv_df.to_csv('/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/top2vec_documents.csv')\n",
        "        topic_csv_df.to_csv('/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/top2vec_topics.csv')\n",
        "\n",
        "\n",
        "class Similarity:\n",
        "    def __init__(self, embedding_path='./embedding/BERTikal'):\n",
        "        self.embedding_path = embedding_path\n",
        "\n",
        "    def compute_cosine_similarity(self, documents=[], list_of_ids=[]):\n",
        "        model = SentenceTransformer(self.embedding_path)\n",
        "        sentences_embeddings = model.encode(documents, show_progress_bar=True)\n",
        "        similarities = cosine_similarity(sentences_embeddings)\n",
        "        distances = []\n",
        "        for r in range(0, len(similarities)):\n",
        "            for c in range(r+1, len(similarities)):\n",
        "                distances.append({'Document_id_1': list_of_ids[r], 'Document_id_2': list_of_ids[c],\n",
        "                                  'Distance': similarities[c][r]})\n",
        "        df = pd.DataFrame(distances)\n",
        "        df.to_csv('distances.csv', sep=',')\n",
        "\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default='top2vec', help=\"Topic modeling algorithm (bertopic or top2vec).\")\n",
        "    return parser.parse_args()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L7Nu68KWehw"
      },
      "source": [
        "##**Main code** - Load data and run the topification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2HXpdwzWd80",
        "outputId": "1281d3c5-5e64-40f9-bbb6-0b4dc4a2691b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents to be removed: 0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(2021)\n",
        "torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = \"bertopic\"    \n",
        "#args = parse_args()\n",
        "\n",
        "### Load Data.\n",
        "ld = LoadData()\n",
        "# load data (documents and lista of ids)\n",
        "documents, list_of_ids = ld.load_data()\n",
        "\n",
        "\n",
        "# pre-process data\n",
        "if model == 'bertopic':\n",
        "    dc = PreprocessingData(min_df=1, max_df=0.75, token_pattern=r\"[a-zA-Z]\\w*\")\n",
        "    preprocessed_corpus, ids_docs_removed = dc.prepare_text_for_topicmodel(documents, list_of_ids)\n",
        "elif model == 'top2vec':\n",
        "    dc = PreprocessingData(min_df=0.01, max_df=0.75, token_pattern=r\"[a-zA-Z]\\w*\")\n",
        "    preprocessed_corpus, ids_docs_removed = dc.prepare_text_for_topicmodel(documents, list_of_ids)\n",
        "else:\n",
        "    print('Model invalid. Running default model - top2vec')\n",
        "    dc = PreprocessingData(min_df=0.01, max_df=0.5, token_pattern=r\"[a-zA-Z]\\w*\")\n",
        "    preprocessed_corpus, ids_docs_removed = dc.prepare_text_for_topicmodel(documents, list_of_ids)\n",
        "\n",
        "print('Number of documents to be removed: %d' % len(ids_docs_removed))\n",
        "if ids_docs_removed:\n",
        "    print('Documents removed: ', ids_docs_removed)\n",
        "    # stats = []\n",
        "for item in ids_docs_removed:\n",
        "    idx = list_of_ids.index(item)\n",
        "    list_of_ids.pop(idx)\n",
        "    documents.pop(idx)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU0TGY0CSDSB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6f1774-8d84-4a29-f215-30b401077b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing bertopic + doc2vec\n",
            "Done creating embeddings.\n",
            "Done initializing bertopic.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-12-01 23:57:52,578 - BERTopic - Reduced dimensionality\n",
            "2022-12-01 23:57:54,470 - BERTopic - Clustered reduced embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Topic  Count\n",
            "0       -1    774\n",
            "1        0    173\n",
            "2        1    127\n",
            "3        2    110\n",
            "4        3     91\n",
            "..     ...    ...\n",
            "113    112     12\n",
            "114    113     12\n",
            "115    114     12\n",
            "116    115     12\n",
            "117    116     11\n",
            "\n",
            "[118 rows x 2 columns]\n",
            "entrou no bert-topic\n"
          ]
        }
      ],
      "source": [
        "# run topic modeling\n",
        "if model == 'bertopic':\n",
        "    tm = TopicModel(corpus=preprocessed_corpus,\n",
        "                    embed_corpus=preprocessed_corpus,\n",
        "                    doc_ids_list=list_of_ids,\n",
        "                    embedding_option='doc2vec',   \n",
        "                    embedding_path='') # PT model: neuralmind/bert-base-portuguese-cased\n",
        "    tm.run_bertopic(calculate_probabilities=True)\n",
        "elif model == 'top2vec':\n",
        "    tm = TopicModel(corpus=preprocessed_corpus,\n",
        "                    doc_ids_list=list_of_ids,\n",
        "                    embedding_option='doc2vec',\n",
        "                    embedding_path='')\n",
        "    tm.run_top2vec()\n",
        "else:\n",
        "    print('Model invalid. Running Bertopic')\n",
        "    tm = TopicModel(corpus=preprocessed_corpus,\n",
        "                    embed_corpus=preprocessed_corpus,\n",
        "                    doc_ids_list=list_of_ids,\n",
        "                    embedding_option='doc2vec',\n",
        "                    embedding_path='') ## Alterar pra rede em ingles se for rodar esse\n",
        "    tm.run_bertopic(calculate_probabilities=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "mn9e9x1U4QIt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWG2Zrdvvyct"
      },
      "source": [
        "## Evaluate topification results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "uCa2mVPasFSK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Index(['Unnamed: 0', 'Document_id', 'Dominant_Topic', 'Topic_Perc_Contrib',\n",
        "#        'Top5_Topics', 'Ementa', 'Q', 'S'], dtye='object')\n",
        "def generate_dominant_map(csv_filepath: str) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    return dict(zip(df.Document_id, df.Dominant_Topic))\n",
        "\n",
        "\n",
        "def generate_top_five_map(csv_filepath: str) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    return dict(zip(df.Document_id, df.Top5_Topics))\n",
        "\n",
        "\n",
        "def generate_document_map(json_file: str) -> map:\n",
        "    with open(json_file) as json_data:\n",
        "        data = json.load(json_data)\n",
        "        return data\n",
        "\n",
        "def generate_top_k_map(csv_filepath: str, k: int) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    result_map = dict(zip(df.Document_id, df.Top5_Topics))\n",
        "    # print(\"Pre-filtering map:\", result_map)\n",
        "    filtered_map = remove_all_elements_greater_than_k(result_map, k)\n",
        "    #print(\"Filtered on top K map:\", filtered_map)\n",
        "    return filtered_map\n",
        "\n",
        "\n",
        "def remove_all_elements_greater_than_k(input_map : map, cut_value: int) -> map:\n",
        "    new_map = dict()\n",
        "    for (key, v) in input_map.items():\n",
        "        l = str_to_list(v)\n",
        "        new_map[key] = build_str(l, cut_value)\n",
        "    return new_map\n",
        "\n",
        "def match_dominant(doc_to_dominant_topic: map, doc_to_refs: map) -> map:\n",
        "    result = Counter()\n",
        "    for (doc, v) in doc_to_refs.items():\n",
        "        for ref in doc_to_refs[doc]:\n",
        "            if (doc_to_dominant_topic[ref] == doc_to_dominant_topic[doc]):\n",
        "                result[doc] = result[doc] + 1\n",
        "        result[doc] = result[doc] / len(doc_to_refs[doc])\n",
        "    return result\n",
        "\n",
        "#ALTERNATIVA 1: Avaliar a presença do tópico dominante da QUERY nos top k tópicos dominantes da RESPOSTA \n",
        "def dominant_inside_top_k_type0(doc_to_dominant_topic: map, doc_to_top_k: map, doc_to_refs: map) -> map:\n",
        "    result = Counter()\n",
        "    for (doc, v) in doc_to_refs.items(): #([('048771.txt', ['038112.txt', '026070.txt']), (...)])\n",
        "      try:\n",
        "        for ref in doc_to_refs[doc]: # ['038112.txt', '026070.txt']\n",
        "          query_dominant = doc_to_dominant_topic[doc] #13\n",
        "          top_k_from_ref = doc_to_top_k[ref].replace(\"[\",\"\").replace(\"]\",\"\").split(' ') # ['94', '5', '105', '3', '20']\n",
        "          if (convert_float_to_int_string(query_dominant) in top_k_from_ref):\n",
        "            result[doc] = result[doc] + 1\n",
        "      except:\n",
        "        result[doc] = result[doc] + 0     \n",
        "      result[doc] = result[doc] / len(doc_to_refs[doc])\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "#ALTERNATIVA 2: #AJUSTAR O doc_to_dominant_topic pra não pegar todos os docs; Avaliar a presença do tópico dominante da RESPOSTA nos top k tópicos dominantes da QUERY ; \n",
        "def dominant_inside_top_k_type2(doc_to_dominant_topic: map, doc_to_top_k: map, doc_to_refs: map) -> map:\n",
        "    result = Counter()\n",
        "    for (doc, v) in doc_to_refs.items(): #([('048771.txt', ['038112.txt', '026070.txt']), (...)])\n",
        "      try:\n",
        "        for ref in doc_to_refs[doc]: # ['038112.txt', '026070.txt']\n",
        "          reference_dominant = doc_to_dominant_topic[ref] #94.0\n",
        "          top_five_from_doc = doc_to_top_k[doc].replace(\"[\",\"\").replace(\"]\",\"\").split(' ') # ['13', '19', '20', '35', '3', '101', '7', '107', '25', '75']\n",
        "          if (convert_float_to_int_string(reference_dominant) in top_five_from_doc):\n",
        "            result[doc] = result[doc] + 1\n",
        "      except:\n",
        "        result[doc] = result[doc] + 0    \n",
        "      result[doc] = result[doc] / len(doc_to_refs[doc])\n",
        "    return result\n",
        "\n",
        "\n",
        "def convert_float_to_int_string(value: float) -> str:\n",
        "    return str(int(float(value)))\n",
        "\n",
        "def str_to_list(value: str) -> list:\n",
        "    cleaned_v = value.replace(\"]\", \"\").replace(\"[\", \"\").strip()\n",
        "    return re.split(\"\\s+\", cleaned_v)\n",
        "\n",
        "\n",
        "def build_str(l: list, cut_value: int) -> str:\n",
        "   return \"[\" + \" \".join(l[0:cut_value]) + \"]\"\n",
        "\n",
        "def references_agreement_on_topic(doc_to_dominant_topic: map, doc_to_refs: map) -> map:\n",
        "    result = Counter()\n",
        "    for (doc, v) in doc_to_refs.items(): #([('048771.txt', ['038112.txt', '026070.txt']), (...)])\n",
        "        tmp_counter = Counter()\n",
        "        for ref in doc_to_refs[doc]:\n",
        "            reference_dominant_value = doc_to_dominant_topic[ref]\n",
        "            tmp_counter[reference_dominant_value] = tmp_counter[reference_dominant_value] + 1\n",
        "        result[doc] = (tmp_counter.most_common(1)[0][0], tmp_counter.most_common(1)[0][1]/len(doc_to_refs[doc]))\n",
        "    return result\n",
        "\n",
        "def write_document_map_to_csv(map: map, filename: str) -> None:\n",
        "    print(\"To write at file: \" + \"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/evaluation/\"+filename)\n",
        "    with open(\"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/evaluation/\"+filename,'w',encoding = 'utf-8') as f:\n",
        "        f.write(\"document,percentual\\n\")\n",
        "        for (k,v) in map.items():\n",
        "            f.write( str(k) + \",\" + str(v) + \"\\n\" )\n",
        "\n",
        "\n",
        "def write_agreement_analysis_to_csv(map: map, filename: str) -> None:\n",
        "    print(\"To write at file: \"+\"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/evaluation/\"+filename)\n",
        "    with open(\"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/evaluation/\"+filename,'w+',encoding = 'utf-8') as f:\n",
        "        f.write(\"document,topic_dominant_id_agreed, percentual_of_the_agreement\\n\")\n",
        "        for (k,v) in map.items():\n",
        "            topic_agreed = convert_float_to_int_string(v[0])\n",
        "            percentual_of_agreement = str(v[1])\n",
        "            f.write( str(k) + \",\" + topic_agreed + \",\"+ percentual_of_agreement + \"\\n\" )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_to_top_k['038112.txt'].replace(\"[\",\"\").replace(\"]\",\"\").split(' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-6Tg_4jz0oE",
        "outputId": "f2bba157-a783-4426-fd37-bb6e300f3d5e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['94', '5', '105', '3', '20']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main code evaluation without threshold and fixed k"
      ],
      "metadata": {
        "id": "y24UkYfK3CDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "uMJAtIFIvxi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ea9511-6d9b-46d5-ddea-f8e626891e44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "References inside Top K:  Counter({'008447.txt': 1.0, '067501.txt': 1.0, '078422.txt': 1.0, '000123.txt': 1.0, '043774.txt': 1.0, '039087.txt': 1.0, '008653.txt': 1.0, '043815.txt': 1.0, '076001.txt': 1.0, '001499.txt': 1.0, '006704.txt': 1.0, '068182.txt': 1.0, '031284.txt': 1.0, '040560.txt': 1.0, '078681.txt': 1.0, '070711.txt': 1.0, '087599.txt': 1.0, '009186.txt': 1.0, '046489.txt': 1.0, '000092.txt': 1.0, '057136.txt': 1.0, '079913.txt': 1.0, '092332.txt': 1.0, '069872.txt': 1.0, '099856.txt': 1.0, '077380.txt': 1.0, '068503.txt': 1.0, '014909.txt': 1.0, '022731.txt': 1.0, '042691.txt': 1.0, '007137.txt': 1.0, '087285.txt': 1.0, '000669.txt': 1.0, '096028.txt': 1.0, '099898.txt': 1.0, '017391.txt': 1.0, '070433.txt': 1.0, '076365.txt': 1.0, '098576.txt': 1.0, '056717.txt': 1.0, '037710.txt': 1.0, '092224.txt': 1.0, '079208.txt': 1.0, '004700.txt': 1.0, '096651.txt': 1.0, '057424.txt': 1.0, '014007.txt': 1.0, '080737.txt': 1.0, '003865.txt': 1.0, '005830.txt': 1.0, '022361.txt': 1.0, '080480.txt': 1.0, '056909.txt': 1.0, '002760.txt': 1.0, '014411.txt': 1.0, '040801.txt': 1.0, '096695.txt': 1.0, '016293.txt': 1.0, '084828.txt': 1.0, '098780.txt': 1.0, '003595.txt': 1.0, '041233.txt': 1.0, '020453.txt': 1.0, '054027.txt': 1.0, '052766.txt': 1.0, '076969.txt': 1.0, '023818.txt': 1.0, '046310.txt': 1.0, '079200.txt': 1.0, '053170.txt': 1.0, '091318.txt': 1.0, '039086.txt': 1.0, '086222.txt': 1.0, '098080.txt': 1.0, '086250.txt': 1.0, '013201.txt': 1.0, '043843.txt': 1.0, '080552.txt': 1.0, '048477.txt': 1.0, '078211.txt': 1.0, '031532.txt': 1.0, '060404.txt': 1.0, '073640.txt': 1.0, '000085.txt': 1.0, '052474.txt': 1.0, '072292.txt': 1.0, '011447.txt': 1.0, '093729.txt': 1.0, '092346.txt': 1.0, '051749.txt': 1.0, '020351.txt': 1.0, '076286.txt': 1.0, '065642.txt': 1.0, '026998.txt': 1.0, '061781.txt': 1.0, '093206.txt': 1.0, '003263.txt': 1.0, '080908.txt': 1.0, '057543.txt': 1.0, '059429.txt': 1.0, '026056.txt': 1.0, '026686.txt': 1.0, '041961.txt': 1.0, '034867.txt': 1.0, '073337.txt': 1.0, '046266.txt': 1.0, '060425.txt': 1.0, '015997.txt': 1.0, '024927.txt': 1.0, '023005.txt': 1.0, '024409.txt': 1.0, '061129.txt': 1.0, '082829.txt': 1.0, '082790.txt': 1.0, '061177.txt': 1.0, '008173.txt': 1.0, '078150.txt': 1.0, '025322.txt': 1.0, '042740.txt': 1.0, '093057.txt': 1.0, '061521.txt': 1.0, '069510.txt': 1.0, '066101.txt': 1.0, '062618.txt': 1.0, '072069.txt': 1.0, '056205.txt': 1.0, '077346.txt': 1.0, '073405.txt': 1.0, '067125.txt': 1.0, '069739.txt': 1.0, '065307.txt': 1.0, '065387.txt': 1.0, '063210.txt': 1.0, '058297.txt': 1.0, '073526.txt': 1.0, '096228.txt': 1.0, '089959.txt': 1.0, '014837.txt': 1.0, '098965.txt': 1.0, '085278.txt': 1.0, '032083.txt': 1.0, '091177.txt': 1.0, '094410.txt': 1.0, '074753.txt': 1.0, '074696.txt': 1.0, '049344.txt': 1.0, '006926.txt': 1.0, '006756.txt': 1.0, '010360.txt': 1.0, '036746.txt': 1.0, '013019.txt': 1.0, '030546.txt': 1.0, '033703.txt': 1.0, '055907.txt': 1.0, '068921.txt': 1.0, '085920.txt': 1.0, '072166.txt': 1.0, '031664.txt': 1.0, '016178.txt': 1.0, '081422.txt': 1.0, '088208.txt': 1.0, '074264.txt': 1.0, '023744.txt': 1.0, '096245.txt': 1.0, '086288.txt': 1.0, '094196.txt': 1.0, '012587.txt': 1.0, '087535.txt': 1.0, '017190.txt': 1.0, '029713.txt': 1.0, '030050.txt': 1.0, '033327.txt': 1.0, '039187.txt': 1.0, '079285.txt': 1.0, '099439.txt': 1.0, '056799.txt': 1.0, '037140.txt': 1.0, '084570.txt': 1.0, '052633.txt': 1.0, '066955.txt': 1.0, '003915.txt': 1.0, '089811.txt': 1.0, '035918.txt': 1.0, '099372.txt': 1.0, '089304.txt': 1.0, '021818.txt': 1.0, '024341.txt': 1.0, '087161.txt': 1.0, '049905.txt': 1.0, '099508.txt': 1.0, '025675.txt': 1.0, '070222.txt': 1.0, '039769.txt': 1.0, '001947.txt': 1.0, '077844.txt': 1.0, '045673.txt': 1.0, '035718.txt': 1.0, '023731.txt': 1.0, '055858.txt': 1.0, '006307.txt': 1.0, '040690.txt': 1.0, '074125.txt': 1.0, '010628.txt': 1.0, '051640.txt': 1.0, '022477.txt': 1.0, '051097.txt': 1.0, '068655.txt': 1.0, '081163.txt': 1.0, '039772.txt': 1.0, '021652.txt': 1.0, '046762.txt': 1.0, '067658.txt': 1.0, '049802.txt': 1.0, '089065.txt': 1.0, '080457.txt': 1.0, '057494.txt': 1.0, '074679.txt': 1.0, '032116.txt': 1.0, '010127.txt': 1.0, '045545.txt': 1.0, '085192.txt': 1.0, '042761.txt': 1.0, '061668.txt': 1.0, '016253.txt': 1.0, '033796.txt': 1.0, '065299.txt': 1.0, '070553.txt': 1.0, '085232.txt': 1.0, '006766.txt': 1.0, '095324.txt': 1.0, '031074.txt': 1.0, '054233.txt': 1.0, '025252.txt': 1.0, '089577.txt': 1.0, '012328.txt': 1.0, '020114.txt': 1.0, '091168.txt': 1.0, '073279.txt': 1.0, '015260.txt': 1.0, '078432.txt': 1.0, '037341.txt': 1.0, '041466.txt': 1.0, '044549.txt': 1.0, '016020.txt': 1.0, '025422.txt': 1.0, '083654.txt': 0.9411764705882353, '095431.txt': 0.9230769230769231, '050332.txt': 0.9230769230769231, '024224.txt': 0.9166666666666666, '036579.txt': 0.9, '085841.txt': 0.8888888888888888, '073494.txt': 0.8888888888888888, '071899.txt': 0.8888888888888888, '098226.txt': 0.875, '049423.txt': 0.875, '006909.txt': 0.875, '097926.txt': 0.875, '064116.txt': 0.875, '010822.txt': 0.875, '002236.txt': 0.875, '087257.txt': 0.875, '031383.txt': 0.875, '007876.txt': 0.8571428571428571, '083086.txt': 0.8571428571428571, '024481.txt': 0.8571428571428571, '001510.txt': 0.8571428571428571, '021995.txt': 0.8571428571428571, '058993.txt': 0.8571428571428571, '066194.txt': 0.8461538461538461, '066278.txt': 0.8421052631578947, '012796.txt': 0.8333333333333334, '051820.txt': 0.8333333333333334, '073784.txt': 0.8333333333333334, '002656.txt': 0.8333333333333334, '052909.txt': 0.8181818181818182, '039621.txt': 0.8181818181818182, '006769.txt': 0.8, '015420.txt': 0.8, '038962.txt': 0.8, '017738.txt': 0.8, '071181.txt': 0.8, '006842.txt': 0.8, '050119.txt': 0.8, '018234.txt': 0.8, '086477.txt': 0.8, '003992.txt': 0.8, '031784.txt': 0.8, '015020.txt': 0.7857142857142857, '097961.txt': 0.7777777777777778, '005628.txt': 0.7777777777777778, '056342.txt': 0.7777777777777778, '060686.txt': 0.7692307692307693, '062394.txt': 0.7692307692307693, '007667.txt': 0.75, '080529.txt': 0.75, '042705.txt': 0.75, '090936.txt': 0.75, '098605.txt': 0.75, '048696.txt': 0.75, '069228.txt': 0.75, '094946.txt': 0.75, '031822.txt': 0.75, '005978.txt': 0.75, '089050.txt': 0.75, '050998.txt': 0.75, '081491.txt': 0.75, '045251.txt': 0.75, '013611.txt': 0.7333333333333333, '038373.txt': 0.7333333333333333, '076011.txt': 0.7272727272727273, '067329.txt': 0.7272727272727273, '017471.txt': 0.7142857142857143, '074927.txt': 0.7142857142857143, '088234.txt': 0.7142857142857143, '000605.txt': 0.7142857142857143, '055154.txt': 0.7142857142857143, '058990.txt': 0.7142857142857143, '052170.txt': 0.7, '079235.txt': 0.7, '069049.txt': 0.7, '048137.txt': 0.7, '028684.txt': 0.7, '022093.txt': 0.6956521739130435, '037117.txt': 0.6666666666666666, '007135.txt': 0.6666666666666666, '058009.txt': 0.6666666666666666, '069976.txt': 0.6666666666666666, '004541.txt': 0.6666666666666666, '062436.txt': 0.6666666666666666, '046080.txt': 0.6666666666666666, '064109.txt': 0.6666666666666666, '012713.txt': 0.6666666666666666, '003218.txt': 0.6666666666666666, '087773.txt': 0.6666666666666666, '005862.txt': 0.6666666666666666, '050528.txt': 0.6666666666666666, '072112.txt': 0.6666666666666666, '057595.txt': 0.6666666666666666, '096679.txt': 0.6666666666666666, '060653.txt': 0.6666666666666666, '060931.txt': 0.6666666666666666, '076138.txt': 0.6666666666666666, '084981.txt': 0.6666666666666666, '036387.txt': 0.6666666666666666, '052609.txt': 0.6666666666666666, '011086.txt': 0.6666666666666666, '074104.txt': 0.6666666666666666, '018383.txt': 0.6666666666666666, '083595.txt': 0.6666666666666666, '030370.txt': 0.6666666666666666, '083142.txt': 0.6666666666666666, '050437.txt': 0.6666666666666666, '081113.txt': 0.6666666666666666, '022823.txt': 0.6666666666666666, '071619.txt': 0.6666666666666666, '040417.txt': 0.6666666666666666, '076483.txt': 0.6666666666666666, '097279.txt': 0.6666666666666666, '005051.txt': 0.6666666666666666, '046349.txt': 0.6666666666666666, '035213.txt': 0.6470588235294118, '003516.txt': 0.6363636363636364, '081803.txt': 0.6363636363636364, '065680.txt': 0.625, '056515.txt': 0.625, '069178.txt': 0.625, '052265.txt': 0.625, '074245.txt': 0.6190476190476191, '074582.txt': 0.6, '061651.txt': 0.6, '022931.txt': 0.6, '024374.txt': 0.6, '005113.txt': 0.6, '093286.txt': 0.6, '059383.txt': 0.6, '067620.txt': 0.6, '002393.txt': 0.6, '025194.txt': 0.6, '016618.txt': 0.5833333333333334, '034622.txt': 0.5833333333333334, '093016.txt': 0.5714285714285714, '011976.txt': 0.5714285714285714, '047695.txt': 0.5714285714285714, '066861.txt': 0.5714285714285714, '039421.txt': 0.5555555555555556, '069632.txt': 0.5555555555555556, '096966.txt': 0.5555555555555556, '069213.txt': 0.5555555555555556, '006304.txt': 0.5555555555555556, '052846.txt': 0.5454545454545454, '036666.txt': 0.5454545454545454, '028359.txt': 0.5384615384615384, '011121.txt': 0.5294117647058824, '089818.txt': 0.5294117647058824, '030394.txt': 0.5, '046467.txt': 0.5, '055208.txt': 0.5, '075527.txt': 0.5, '079028.txt': 0.5, '031641.txt': 0.5, '079849.txt': 0.5, '081689.txt': 0.5, '078779.txt': 0.5, '016058.txt': 0.5, '099187.txt': 0.5, '064936.txt': 0.5, '018440.txt': 0.5, '071437.txt': 0.5, '016438.txt': 0.5, '099551.txt': 0.5, '037011.txt': 0.5, '063855.txt': 0.5, '019821.txt': 0.5, '072315.txt': 0.5, '000393.txt': 0.5, '083380.txt': 0.5, '048230.txt': 0.5, '029207.txt': 0.5, '065028.txt': 0.5, '086570.txt': 0.5, '092144.txt': 0.5, '079773.txt': 0.5, '007365.txt': 0.5, '076684.txt': 0.5, '055104.txt': 0.5, '032353.txt': 0.5, '030315.txt': 0.5, '020951.txt': 0.5, '027316.txt': 0.5, '093214.txt': 0.5, '010024.txt': 0.5, '026185.txt': 0.5, '085737.txt': 0.5, '052454.txt': 0.5, '030675.txt': 0.5, '021601.txt': 0.5, '098353.txt': 0.5, '097987.txt': 0.5, '045104.txt': 0.5, '006113.txt': 0.5, '074300.txt': 0.5, '007283.txt': 0.5, '087656.txt': 0.5, '021797.txt': 0.5, '027503.txt': 0.5, '049599.txt': 0.5, '097970.txt': 0.5, '073000.txt': 0.45454545454545453, '053848.txt': 0.4444444444444444, '069868.txt': 0.4444444444444444, '076685.txt': 0.4444444444444444, '000031.txt': 0.4444444444444444, '076491.txt': 0.42857142857142855, '045787.txt': 0.42857142857142855, '048376.txt': 0.42857142857142855, '019275.txt': 0.42857142857142855, '002770.txt': 0.42857142857142855, '099814.txt': 0.42857142857142855, '092127.txt': 0.42857142857142855, '014510.txt': 0.42857142857142855, '045061.txt': 0.42857142857142855, '001153.txt': 0.42857142857142855, '056002.txt': 0.42857142857142855, '046017.txt': 0.4, '070873.txt': 0.4, '019591.txt': 0.4, '099124.txt': 0.4, '085279.txt': 0.4, '074006.txt': 0.4, '020165.txt': 0.3888888888888889, '037853.txt': 0.3870967741935484, '099064.txt': 0.38095238095238093, '078963.txt': 0.375, '000418.txt': 0.36363636363636365, '031853.txt': 0.36363636363636365, '075817.txt': 0.36363636363636365, '031183.txt': 0.3333333333333333, '008771.txt': 0.3333333333333333, '029645.txt': 0.3333333333333333, '005409.txt': 0.3333333333333333, '036162.txt': 0.3333333333333333, '000447.txt': 0.3333333333333333, '035550.txt': 0.3333333333333333, '073660.txt': 0.3333333333333333, '004066.txt': 0.3333333333333333, '044675.txt': 0.3333333333333333, '065832.txt': 0.3333333333333333, '043608.txt': 0.3333333333333333, '004471.txt': 0.3333333333333333, '082631.txt': 0.3333333333333333, '095429.txt': 0.3333333333333333, '076640.txt': 0.3333333333333333, '052168.txt': 0.3333333333333333, '024179.txt': 0.3333333333333333, '089468.txt': 0.3333333333333333, '015055.txt': 0.3333333333333333, '013029.txt': 0.3333333333333333, '008882.txt': 0.3333333333333333, '031336.txt': 0.3333333333333333, '083224.txt': 0.3333333333333333, '049784.txt': 0.3333333333333333, '097277.txt': 0.32, '060807.txt': 0.3, '028914.txt': 0.2857142857142857, '035016.txt': 0.2857142857142857, '033096.txt': 0.2857142857142857, '050486.txt': 0.2857142857142857, '083342.txt': 0.2727272727272727, '061878.txt': 0.2727272727272727, '046955.txt': 0.2727272727272727, '090592.txt': 0.2727272727272727, '088711.txt': 0.25, '050190.txt': 0.25, '024157.txt': 0.25, '027864.txt': 0.25, '069675.txt': 0.25, '002314.txt': 0.25, '089448.txt': 0.25, '076003.txt': 0.25, '093797.txt': 0.25, '024429.txt': 0.25, '034805.txt': 0.25, '011346.txt': 0.25, '043560.txt': 0.25, '068739.txt': 0.25, '017617.txt': 0.25, '098981.txt': 0.25, '031140.txt': 0.25, '039860.txt': 0.25, '028416.txt': 0.23076923076923078, '001884.txt': 0.2, '077071.txt': 0.2, '036382.txt': 0.2, '002669.txt': 0.2, '041293.txt': 0.2, '089515.txt': 0.2, '057191.txt': 0.2, '016748.txt': 0.2, '066012.txt': 0.2, '089555.txt': 0.18181818181818182, '024269.txt': 0.16666666666666666, '056324.txt': 0.16666666666666666, '007698.txt': 0.16666666666666666, '050237.txt': 0.16666666666666666, '020090.txt': 0.16666666666666666, '015966.txt': 0.14285714285714285, '037809.txt': 0.14285714285714285, '055216.txt': 0.14285714285714285, '002716.txt': 0.14285714285714285, '007286.txt': 0.125, '076477.txt': 0.125, '022534.txt': 0.125, '039301.txt': 0.1111111111111111, '094198.txt': 0.1111111111111111, '085782.txt': 0.1, '017699.txt': 0.08333333333333333, '015161.txt': 0.07692307692307693, '007627.txt': 0.0, '004108.txt': 0.0, '071236.txt': 0.0, '038947.txt': 0.0, '015031.txt': 0.0, '076990.txt': 0.0, '015327.txt': 0.0, '024648.txt': 0.0, '081919.txt': 0.0, '040239.txt': 0.0, '026213.txt': 0.0, '069092.txt': 0.0, '011450.txt': 0.0, '055693.txt': 0.0, '060780.txt': 0.0, '046478.txt': 0.0, '056962.txt': 0.0, '048500.txt': 0.0, '062926.txt': 0.0, '037440.txt': 0.0, '079672.txt': 0.0, '064402.txt': 0.0, '048771.txt': 0.0, '055546.txt': 0.0, '043523.txt': 0.0, '056963.txt': 0.0, '084496.txt': 0.0, '087884.txt': 0.0, '023554.txt': 0.0, '097067.txt': 0.0, '055968.txt': 0.0, '081675.txt': 0.0, '052702.txt': 0.0, '021820.txt': 0.0, '002449.txt': 0.0, '065127.txt': 0.0, '048888.txt': 0.0, '020040.txt': 0.0, '016165.txt': 0.0, '050729.txt': 0.0, '002648.txt': 0.0, '045846.txt': 0.0, '040011.txt': 0.0, '080135.txt': 0.0, '066880.txt': 0.0, '078583.txt': 0.0, '082393.txt': 0.0, '049750.txt': 0.0, '057990.txt': 0.0, '080304.txt': 0.0, '067346.txt': 0.0, '017025.txt': 0.0, '092690.txt': 0.0, '042545.txt': 0.0, '033823.txt': 0.0, '060132.txt': 0.0, '005267.txt': 0.0, '095993.txt': 0.0, '006922.txt': 0.0, '035994.txt': 0.0, '032248.txt': 0.0, '012111.txt': 0.0, '043298.txt': 0.0, '037221.txt': 0.0, '041390.txt': 0.0, '050666.txt': 0.0, '027247.txt': 0.0, '049261.txt': 0.0, '022160.txt': 0.0, '042957.txt': 0.0, '061196.txt': 0.0, '036157.txt': 0.0, '097104.txt': 0.0, '050382.txt': 0.0, '023107.txt': 0.0, '017701.txt': 0.0, '031872.txt': 0.0, '088672.txt': 0.0, '094548.txt': 0.0, '090433.txt': 0.0, '005768.txt': 0.0, '024403.txt': 0.0, '004492.txt': 0.0, '004933.txt': 0.0, '099706.txt': 0.0, '086163.txt': 0.0, '003681.txt': 0.0, '009309.txt': 0.0, '030557.txt': 0.0, '001127.txt': 0.0, '005430.txt': 0.0, '024022.txt': 0.0, '033709.txt': 0.0}) \n",
            "\n",
            "0.6316335407379728\n"
          ]
        }
      ],
      "source": [
        "# Input files\n",
        "bert_result_file = \"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/results/bertopic_documents_final-k-max.csv\"\n",
        "document_references_file = \"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/task1_train_labels.json\"\n",
        "\n",
        "# Compute the maps for analysis\n",
        "#dict: doc -> dominant topic ; 4415 items; {'021359.txt': 20.0, '028184.txt': 0.0 ....}\n",
        "doc_to_dominant_topic = generate_dominant_map(bert_result_file)\n",
        "#dict: queries -> answers ; 650 items; {'008447.txt': ['072495.txt', '082291.txt',...]...}\n",
        "doc_to_refs = generate_document_map(document_references_file) \n",
        "#dict: doc -> top K dominant topics ; 4415 items; {'021359.txt': '[20 3 19 13 105]', ...}\n",
        "doc_to_top_k_raw = generate_top_k_map(bert_result_file,5) \n",
        "\n",
        "# Generate analysis\n",
        "import statistics\n",
        "\n",
        "# 1. Recall (Dominant Match): % of the references which has the same dominant as the original document\n",
        "# dominant_matches = match_dominant(doc_to_dominant_topic, doc_to_refs)\n",
        "# print(\"Recall: \", dominant_matches)\n",
        "# mean = statistics.mean(dominant_matches.values())\n",
        "# print(mean)\n",
        "# write_document_map_to_csv(dominant_matches, \"Recall_bertopic_final.csv\")\n",
        "\n",
        "# 2. Topic Agreement on References: % of references which have the same dominant topic\n",
        "#   agreement_result = references_agreement_on_topic(doc_to_dominant_topic, doc_to_refs)\n",
        "# print(\"Topic Agreement on References: \", agreement_result, '\\n')\n",
        "# write_agreement_analysis_to_csv(agreement_result, \"agreement_result_bertopic_final.csv\")\n",
        "\n",
        "# 3. Recall top K (References inside Top 5): % of reference dominant topics which is inside the document top five\n",
        "Recall_topK = dominant_inside_top_k_type2(doc_to_dominant_topic, doc_to_top_k_raw, doc_to_refs)\n",
        "print(\"References inside Top K: \", Recall_topK, '\\n')\n",
        "\n",
        "mean3 = statistics.mean(Recall_topK.values())\n",
        "print(mean3)\n",
        "#   write_document_map_to_csv(refs_inside_top_k, \"top_20_bertopic_final.csv\")\n",
        "\n",
        "\n",
        "#4. Precision : number of references in the same dominant topic of the query / total number of docs in this topic\n",
        "#precision_results = precision(doc_to_dominant_topic, doc_to_refs, precision_docs)\n",
        "#print(\"Precision: \", precision_results)\n",
        "#write_document_map_to_csv(precision_results, \"precision_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding a threshold\n",
        "\n"
      ],
      "metadata": {
        "id": "ITj31MTG5vpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statistics\n",
        "from collections import Counter\n",
        "import json\n",
        "import re\n",
        "\n",
        "#load docs\n",
        "bert_result_file = \"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/results/bertopic_documents_final-k-max.csv\"\n",
        "document_references_file = \"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/task1_train_labels.json\"\n",
        "queries_to_dominant_topic = \"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/queries.csv\"\n",
        "\n",
        "#Select >>>>candidate topics<<<< considering the Perc of Contrib from the dominant topic > threshould\n",
        "#Other option would be to put this threshold when definiing the doc_to_dominant_topic map; teríamos casos em que nao iamos ter nenhum doc dominante, Nesses casos poderíamos usar similaridade de coseno\n",
        "\n",
        "def generate_top_k_map_threshold(csv_filepath: str, k: int, threshold) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    result_list_raw = (pd.DataFrame(zip(df.Document_id, df.Topic_Perc_Contrib, df.Top5_Topics), columns = ['Document_id', 'Topic_Perc_Contrib', 'Top5_Topics']))\n",
        "    #print(\"Pre-filtering map:\", result_list_raw)\n",
        "    result_after_threshold = result_list_raw[result_list_raw['Topic_Perc_Contrib'] > threshold]\n",
        "    result_map = dict(zip(result_after_threshold['Document_id'],result_after_threshold['Top5_Topics']))\n",
        "    #INSERT QUERY DOCS BACK\n",
        "    query_map = generate_top_k_map(queries_to_dominant_topic,k)\n",
        "    final_map = {**query_map,**result_map}\n",
        "    #print(\"Filtered on top K map:\", result_map)\n",
        "    filtered_map = remove_all_elements_greater_than_k(final_map, k)\n",
        "    #print(\"Filtered on top K map:\", filtered_map)\n",
        "    return filtered_map\n",
        "\n",
        "\n",
        "#this one without a threshould\n",
        "def generate_top_k_map(csv_filepath: str, k: int) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    result_map = dict(zip(df.Document_id, df.Top5_Topics)) # TOP K\n",
        "    # print(\"Pre-filtering map:\", result_map)\n",
        "    filtered_map = remove_all_elements_greater_than_k(result_map, k)\n",
        "    # print(\"Filtered on top K map:\", filtered_map)\n",
        "    return filtered_map\n",
        "\n",
        "\n",
        "\n",
        "def remove_all_elements_greater_than_k(input_map : map, cut_value: int):\n",
        "    new_map = dict()\n",
        "    for (key, v) in input_map.items():\n",
        "        l = str_to_list(v)\n",
        "        new_map[key] = build_str(l, cut_value)\n",
        "    return new_map\n",
        "\n",
        "def str_to_list(value: str) -> list:\n",
        "    cleaned_v = value.replace(\"]\", \"\").replace(\"[\", \"\").strip()\n",
        "    return re.split(\"\\s+\", cleaned_v)\n",
        "\n",
        "def build_str(l: list, cut_value: int) -> str:\n",
        "   return \"[\" + \" \".join(l[0:cut_value]) + \"]\"\n",
        "\n",
        "def generate_dominant_map(csv_filepath: str) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    return dict(zip(df.Document_id, df.Dominant_Topic))\n",
        "\n",
        "def generate_document_map(json_file: str) -> map:\n",
        "    with open(json_file) as json_data:\n",
        "        data = json.load(json_data)\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "5XrG_XNF_ksZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation with threshold and fixed K"
      ],
      "metadata": {
        "id": "4jDzMsrGDsWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For k = 10\n",
        "k=10\n",
        "threshold= 0.2\n",
        "\n",
        "## Compute the maps for analysis\n",
        "#dict: doc -> dominant topic ; 4415 items; {'021359.txt': 20.0, '028184.txt': 0.0 ....}\n",
        "doc_to_dominant_topic = generate_dominant_map(bert_result_file)\n",
        "\n",
        "#dict: queries -> answers ; 650 items; {'008447.txt': ['072495.txt', '082291.txt',...]...}\n",
        "doc_to_refs = generate_document_map(document_references_file) \n",
        "\n",
        "#dict: doc -> top K=5 dominant topics, threhold = 0.2 ;  items; {'021359.txt': '[20 3 19 13 105]', ...}\n",
        "doc_to_top_k_raw = generate_top_k_map_threshold(bert_result_file,k,threshold)\n",
        "\n",
        "\n",
        "# Generate analysis\n",
        "import statistics\n",
        "\n",
        "#Recall top K (References inside Top 5): % of reference dominant topics which is inside the document top five\n",
        "refs_inside_top_k = dominant_inside_top_k_type0(doc_to_dominant_topic, doc_to_top_k_raw, doc_to_refs)\n",
        "\n",
        "\n",
        "\n",
        "print(\"References inside Top K: \", refs_inside_top_k, '\\n')\n",
        "mean3 = statistics.mean(refs_inside_top_k.values())\n",
        "median = statistics.median(refs_inside_top_k.values())\n",
        "print(mean3)\n",
        "print(median)\n",
        "print(refs_inside_top_k)\n",
        "\n",
        "#write_document_map_to_csv(refs_inside_top_k, \"NEW-top_10_with_x=02.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oExhMf_Dzwp",
        "outputId": "453c00ed-0937-4170-afca-164ee2d5b065"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "References inside Top K:  Counter({'008447.txt': 1.0, '043815.txt': 1.0, '001499.txt': 1.0, '068182.txt': 1.0, '078681.txt': 1.0, '070711.txt': 1.0, '046489.txt': 1.0, '000092.txt': 1.0, '057136.txt': 1.0, '079913.txt': 1.0, '092332.txt': 1.0, '069872.txt': 1.0, '099856.txt': 1.0, '077380.txt': 1.0, '022731.txt': 1.0, '070433.txt': 1.0, '076365.txt': 1.0, '098576.txt': 1.0, '056717.txt': 1.0, '092224.txt': 1.0, '004700.txt': 1.0, '014007.txt': 1.0, '003865.txt': 1.0, '005830.txt': 1.0, '096695.txt': 1.0, '016293.txt': 1.0, '084828.txt': 1.0, '003595.txt': 1.0, '041233.txt': 1.0, '052766.txt': 1.0, '053170.txt': 1.0, '091318.txt': 1.0, '055546.txt': 1.0, '086222.txt': 1.0, '013201.txt': 1.0, '080552.txt': 1.0, '048477.txt': 1.0, '078211.txt': 1.0, '031532.txt': 1.0, '000085.txt': 1.0, '052474.txt': 1.0, '092346.txt': 1.0, '076286.txt': 1.0, '065642.txt': 1.0, '026998.txt': 1.0, '061781.txt': 1.0, '003263.txt': 1.0, '059429.txt': 1.0, '034867.txt': 1.0, '060425.txt': 1.0, '024927.txt': 1.0, '023005.txt': 1.0, '024409.txt': 1.0, '061129.txt': 1.0, '082790.txt': 1.0, '078150.txt': 1.0, '025322.txt': 1.0, '093057.txt': 1.0, '061521.txt': 1.0, '069510.txt': 1.0, '066101.txt': 1.0, '062618.txt': 1.0, '077346.txt': 1.0, '073405.txt': 1.0, '069739.txt': 1.0, '096228.txt': 1.0, '089959.txt': 1.0, '014837.txt': 1.0, '032083.txt': 1.0, '091177.txt': 1.0, '049344.txt': 1.0, '006926.txt': 1.0, '006756.txt': 1.0, '036746.txt': 1.0, '055907.txt': 1.0, '068921.txt': 1.0, '031664.txt': 1.0, '016178.txt': 1.0, '023744.txt': 1.0, '096245.txt': 1.0, '094196.txt': 1.0, '012587.txt': 1.0, '087535.txt': 1.0, '017190.txt': 1.0, '029713.txt': 1.0, '050998.txt': 1.0, '033327.txt': 1.0, '056799.txt': 1.0, '084570.txt': 1.0, '099372.txt': 1.0, '024341.txt': 1.0, '087161.txt': 1.0, '025675.txt': 1.0, '070222.txt': 1.0, '039769.txt': 1.0, '045673.txt': 1.0, '035718.txt': 1.0, '012111.txt': 1.0, '074125.txt': 1.0, '010628.txt': 1.0, '051640.txt': 1.0, '022477.txt': 1.0, '081113.txt': 1.0, '081163.txt': 1.0, '022823.txt': 1.0, '040417.txt': 1.0, '057494.txt': 1.0, '010127.txt': 1.0, '070553.txt': 1.0, '006766.txt': 1.0, '025252.txt': 1.0, '012328.txt': 1.0, '091168.txt': 1.0, '073279.txt': 1.0, '078432.txt': 1.0, '041466.txt': 1.0, '033709.txt': 1.0, '093729.txt': 0.8571428571428571, '066194.txt': 0.8461538461538461, '076001.txt': 0.8333333333333334, '026056.txt': 0.8333333333333334, '051820.txt': 0.8333333333333334, '052633.txt': 0.8333333333333334, '006769.txt': 0.8, '006842.txt': 0.8, '002393.txt': 0.8, '055154.txt': 0.7857142857142857, '007667.txt': 0.75, '060404.txt': 0.75, '061177.txt': 0.75, '055858.txt': 0.75, '006307.txt': 0.75, '031383.txt': 0.75, '045251.txt': 0.75, '074927.txt': 0.7142857142857143, '079235.txt': 0.7, '007135.txt': 0.6666666666666666, '014909.txt': 0.6666666666666666, '024648.txt': 0.6666666666666666, '050528.txt': 0.6666666666666666, '060653.txt': 0.6666666666666666, '018383.txt': 0.6666666666666666, '030370.txt': 0.6666666666666666, '042545.txt': 0.6666666666666666, '071619.txt': 0.6666666666666666, '085232.txt': 0.6666666666666666, '005051.txt': 0.6666666666666666, '048696.txt': 0.625, '008653.txt': 0.6, '015420.txt': 0.6, '093286.txt': 0.6, '001510.txt': 0.5714285714285714, '011976.txt': 0.5714285714285714, '058297.txt': 0.5714285714285714, '074696.txt': 0.5714285714285714, '081422.txt': 0.5714285714285714, '099814.txt': 0.5714285714285714, '092127.txt': 0.5714285714285714, '000605.txt': 0.5714285714285714, '078422.txt': 0.5, '046467.txt': 0.5, '042691.txt': 0.5, '069092.txt': 0.5, '096651.txt': 0.5, '079849.txt': 0.5, '018440.txt': 0.5, '043843.txt': 0.5, '056963.txt': 0.5, '073640.txt': 0.5, '000393.txt': 0.5, '083380.txt': 0.5, '094946.txt': 0.5, '048230.txt': 0.5, '045846.txt': 0.5, '079773.txt': 0.5, '030315.txt': 0.5, '077844.txt': 0.5, '097987.txt': 0.5, '007283.txt': 0.5, '097961.txt': 0.4444444444444444, '079208.txt': 0.42857142857142855, '045787.txt': 0.42857142857142855, '080480.txt': 0.4, '001884.txt': 0.4, '018234.txt': 0.4, '040690.txt': 0.4, '068655.txt': 0.4, '025194.txt': 0.4, '065680.txt': 0.375, '064936.txt': 0.375, '063855.txt': 0.375, '098981.txt': 0.375, '031183.txt': 0.3333333333333333, '039087.txt': 0.3333333333333333, '031284.txt': 0.3333333333333333, '039421.txt': 0.3333333333333333, '017391.txt': 0.3333333333333333, '036162.txt': 0.3333333333333333, '064109.txt': 0.3333333333333333, '069632.txt': 0.3333333333333333, '024224.txt': 0.3333333333333333, '056342.txt': 0.3333333333333333, '098965.txt': 0.3333333333333333, '084981.txt': 0.3333333333333333, '036387.txt': 0.3333333333333333, '082631.txt': 0.3333333333333333, '095429.txt': 0.3333333333333333, '079285.txt': 0.3333333333333333, '037140.txt': 0.3333333333333333, '050437.txt': 0.3333333333333333, '008882.txt': 0.3333333333333333, '031336.txt': 0.3333333333333333, '089065.txt': 0.3333333333333333, '083224.txt': 0.3333333333333333, '050237.txt': 0.3333333333333333, '052170.txt': 0.3, '048137.txt': 0.3, '020951.txt': 0.3, '007876.txt': 0.2857142857142857, '045545.txt': 0.2857142857142857, '039621.txt': 0.2727272727272727, '090592.txt': 0.2727272727272727, '000123.txt': 0.25, '043774.txt': 0.25, '000669.txt': 0.25, '080529.txt': 0.25, '049423.txt': 0.25, '016438.txt': 0.25, '069675.txt': 0.25, '098605.txt': 0.25, '031822.txt': 0.25, '056515.txt': 0.25, '099439.txt': 0.25, '043560.txt': 0.25, '002236.txt': 0.25, '046762.txt': 0.25, '017617.txt': 0.25, '083654.txt': 0.23529411764705882, '050332.txt': 0.23076923076923078, '067658.txt': 0.2222222222222222, '076483.txt': 0.2222222222222222, '046349.txt': 0.2222222222222222, '066278.txt': 0.21052631578947367, '054027.txt': 0.2, '077071.txt': 0.2, '082829.txt': 0.2, '021820.txt': 0.2, '036382.txt': 0.2, '002669.txt': 0.2, '005113.txt': 0.2, '049905.txt': 0.2, '028684.txt': 0.2, '067620.txt': 0.2, '089515.txt': 0.2, '086477.txt': 0.2, '066012.txt': 0.2, '081803.txt': 0.18181818181818182, '035918.txt': 0.18181818181818182, '035213.txt': 0.17647058823529413, '073494.txt': 0.16666666666666666, '005862.txt': 0.16666666666666666, '057595.txt': 0.16666666666666666, '012796.txt': 0.16666666666666666, '004066.txt': 0.16666666666666666, '024269.txt': 0.16666666666666666, '020165.txt': 0.16666666666666666, '095431.txt': 0.15384615384615385, '037809.txt': 0.14285714285714285, '048376.txt': 0.14285714285714285, '033096.txt': 0.14285714285714285, '002770.txt': 0.14285714285714285, '021995.txt': 0.14285714285714285, '058993.txt': 0.14285714285714285, '088234.txt': 0.14285714285714285, '014510.txt': 0.14285714285714285, '045061.txt': 0.14285714285714285, '058990.txt': 0.14285714285714285, '056002.txt': 0.14285714285714285, '058009.txt': 0.13333333333333333, '038373.txt': 0.13333333333333333, '075527.txt': 0.125, '006909.txt': 0.125, '064116.txt': 0.125, '010822.txt': 0.125, '067125.txt': 0.125, '076003.txt': 0.125, '021601.txt': 0.125, '053848.txt': 0.1111111111111111, '085841.txt': 0.1111111111111111, '069868.txt': 0.1111111111111111, '015055.txt': 0.1111111111111111, '006304.txt': 0.1111111111111111, '015997.txt': 0.1, '029207.txt': 0.1, '069049.txt': 0.1, '099064.txt': 0.09523809523809523, '000418.txt': 0.09090909090909091, '003516.txt': 0.09090909090909091, '031853.txt': 0.09090909090909091, '076011.txt': 0.09090909090909091, '073000.txt': 0.09090909090909091, '046955.txt': 0.09090909090909091, '004541.txt': 0.08333333333333333, '087285.txt': 0.08333333333333333, '060686.txt': 0.07692307692307693, '062394.txt': 0.07692307692307693, '015020.txt': 0.07142857142857142, '027316.txt': 0.07142857142857142, '011121.txt': 0.058823529411764705, '016020.txt': 0.058823529411764705, '093016.txt': 0.047619047619047616, '022093.txt': 0.043478260869565216, '067501.txt': 0.0, '007627.txt': 0.0, '030394.txt': 0.0, '006704.txt': 0.0, '004108.txt': 0.0, '040560.txt': 0.0, '037117.txt': 0.0, '016618.txt': 0.0, '088711.txt': 0.0, '050190.txt': 0.0, '060807.txt': 0.0, '046017.txt': 0.0, '071236.txt': 0.0, '008771.txt': 0.0, '087599.txt': 0.0, '009186.txt': 0.0, '038947.txt': 0.0, '015031.txt': 0.0, '074582.txt': 0.0, '089555.txt': 0.0, '015966.txt': 0.0, '083086.txt': 0.0, '076990.txt': 0.0, '024157.txt': 0.0, '024481.txt': 0.0, '098226.txt': 0.0, '069976.txt': 0.0, '029645.txt': 0.0, '068503.txt': 0.0, '015327.txt': 0.0, '070873.txt': 0.0, '055208.txt': 0.0, '005409.txt': 0.0, '007137.txt': 0.0, '061651.txt': 0.0, '081919.txt': 0.0, '040239.txt': 0.0, '062436.txt': 0.0, '079028.txt': 0.0, '076491.txt': 0.0, '046080.txt': 0.0, '026213.txt': 0.0, '096028.txt': 0.0, '099898.txt': 0.0, '015161.txt': 0.0, '011450.txt': 0.0, '055693.txt': 0.0, '037710.txt': 0.0, '060780.txt': 0.0, '000447.txt': 0.0, '017471.txt': 0.0, '046478.txt': 0.0, '031641.txt': 0.0, '097277.txt': 0.0, '057424.txt': 0.0, '056962.txt': 0.0, '022931.txt': 0.0, '080737.txt': 0.0, '022361.txt': 0.0, '048500.txt': 0.0, '056909.txt': 0.0, '002760.txt': 0.0, '012713.txt': 0.0, '014411.txt': 0.0, '040801.txt': 0.0, '062926.txt': 0.0, '027864.txt': 0.0, '098780.txt': 0.0, '003218.txt': 0.0, '081689.txt': 0.0, '042705.txt': 0.0, '087773.txt': 0.0, '078779.txt': 0.0, '016058.txt': 0.0, '037440.txt': 0.0, '083342.txt': 0.0, '037853.txt': 0.0, '099187.txt': 0.0, '079672.txt': 0.0, '064402.txt': 0.0, '020453.txt': 0.0, '076969.txt': 0.0, '023818.txt': 0.0, '046310.txt': 0.0, '048771.txt': 0.0, '052909.txt': 0.0, '079200.txt': 0.0, '039086.txt': 0.0, '043523.txt': 0.0, '098080.txt': 0.0, '028359.txt': 0.0, '097926.txt': 0.0, '072112.txt': 0.0, '086250.txt': 0.0, '071437.txt': 0.0, '036579.txt': 0.0, '090936.txt': 0.0, '099551.txt': 0.0, '096679.txt': 0.0, '072292.txt': 0.0, '028914.txt': 0.0, '037011.txt': 0.0, '069228.txt': 0.0, '038962.txt': 0.0, '011447.txt': 0.0, '051749.txt': 0.0, '005628.txt': 0.0, '020351.txt': 0.0, '019821.txt': 0.0, '072315.txt': 0.0, '093206.txt': 0.0, '084496.txt': 0.0, '035016.txt': 0.0, '080908.txt': 0.0, '057543.txt': 0.0, '035550.txt': 0.0, '071899.txt': 0.0, '087884.txt': 0.0, '034622.txt': 0.0, '026686.txt': 0.0, '041961.txt': 0.0, '060931.txt': 0.0, '073337.txt': 0.0, '046266.txt': 0.0, '019591.txt': 0.0, '017738.txt': 0.0, '099124.txt': 0.0, '076138.txt': 0.0, '023554.txt': 0.0, '097067.txt': 0.0, '065028.txt': 0.0, '055968.txt': 0.0, '008173.txt': 0.0, '073660.txt': 0.0, '042740.txt': 0.0, '013611.txt': 0.0, '081675.txt': 0.0, '002314.txt': 0.0, '089448.txt': 0.0, '052702.txt': 0.0, '071181.txt': 0.0, '061878.txt': 0.0, '072069.txt': 0.0, '056205.txt': 0.0, '002449.txt': 0.0, '076685.txt': 0.0, '065127.txt': 0.0, '065307.txt': 0.0, '065387.txt': 0.0, '063210.txt': 0.0, '073526.txt': 0.0, '086570.txt': 0.0, '044675.txt': 0.0, '048888.txt': 0.0, '024374.txt': 0.0, '085278.txt': 0.0, '065832.txt': 0.0, '020040.txt': 0.0, '052609.txt': 0.0, '094410.txt': 0.0, '019275.txt': 0.0, '074753.txt': 0.0, '011086.txt': 0.0, '073784.txt': 0.0, '010360.txt': 0.0, '043608.txt': 0.0, '013019.txt': 0.0, '005978.txt': 0.0, '017699.txt': 0.0, '052846.txt': 0.0, '030546.txt': 0.0, '016165.txt': 0.0, '050729.txt': 0.0, '033703.txt': 0.0, '002648.txt': 0.0, '040011.txt': 0.0, '004471.txt': 0.0, '092144.txt': 0.0, '074104.txt': 0.0, '080135.txt': 0.0, '085920.txt': 0.0, '066880.txt': 0.0, '072166.txt': 0.0, '007365.txt': 0.0, '078583.txt': 0.0, '082393.txt': 0.0, '049750.txt': 0.0, '076684.txt': 0.0, '002656.txt': 0.0, '057990.txt': 0.0, '076640.txt': 0.0, '088208.txt': 0.0, '074264.txt': 0.0, '055104.txt': 0.0, '032353.txt': 0.0, '083595.txt': 0.0, '000031.txt': 0.0, '089050.txt': 0.0, '086288.txt': 0.0, '050119.txt': 0.0, '080304.txt': 0.0, '067346.txt': 0.0, '030050.txt': 0.0, '069178.txt': 0.0, '039187.txt': 0.0, '052168.txt': 0.0, '024179.txt': 0.0, '056324.txt': 0.0, '083142.txt': 0.0, '017025.txt': 0.0, '092690.txt': 0.0, '085279.txt': 0.0, '085782.txt': 0.0, '041293.txt': 0.0, '093797.txt': 0.0, '024429.txt': 0.0, '066955.txt': 0.0, '034805.txt': 0.0, '093214.txt': 0.0, '096966.txt': 0.0, '003915.txt': 0.0, '089811.txt': 0.0, '055216.txt': 0.0, '033823.txt': 0.0, '089304.txt': 0.0, '060132.txt': 0.0, '021818.txt': 0.0, '059383.txt': 0.0, '099508.txt': 0.0, '089468.txt': 0.0, '011346.txt': 0.0, '005267.txt': 0.0, '039301.txt': 0.0, '095993.txt': 0.0, '010024.txt': 0.0, '006922.txt': 0.0, '007286.txt': 0.0, '026185.txt': 0.0, '067329.txt': 0.0, '001947.txt': 0.0, '035994.txt': 0.0, '032248.txt': 0.0, '074245.txt': 0.0, '068739.txt': 0.0, '085737.txt': 0.0, '023731.txt': 0.0, '081491.txt': 0.0, '052454.txt': 0.0, '030675.txt': 0.0, '069213.txt': 0.0, '051097.txt': 0.0, '013029.txt': 0.0, '043298.txt': 0.0, '037221.txt': 0.0, '041390.txt': 0.0, '057191.txt': 0.0, '050666.txt': 0.0, '007698.txt': 0.0, '027247.txt': 0.0, '028416.txt': 0.0, '049261.txt': 0.0, '022160.txt': 0.0, '098353.txt': 0.0, '087257.txt': 0.0, '042957.txt': 0.0, '039772.txt': 0.0, '061196.txt': 0.0, '075817.txt': 0.0, '016748.txt': 0.0, '021652.txt': 0.0, '045104.txt': 0.0, '006113.txt': 0.0, '074300.txt': 0.0, '036157.txt': 0.0, '047695.txt': 0.0, '049802.txt': 0.0, '097104.txt': 0.0, '002716.txt': 0.0, '080457.txt': 0.0, '003992.txt': 0.0, '050382.txt': 0.0, '076477.txt': 0.0, '049784.txt': 0.0, '023107.txt': 0.0, '017701.txt': 0.0, '074679.txt': 0.0, '001153.txt': 0.0, '022534.txt': 0.0, '031872.txt': 0.0, '032116.txt': 0.0, '078963.txt': 0.0, '088672.txt': 0.0, '074006.txt': 0.0, '094198.txt': 0.0, '085192.txt': 0.0, '042761.txt': 0.0, '061668.txt': 0.0, '016253.txt': 0.0, '094548.txt': 0.0, '033796.txt': 0.0, '090433.txt': 0.0, '065299.txt': 0.0, '050486.txt': 0.0, '005768.txt': 0.0, '087656.txt': 0.0, '020090.txt': 0.0, '024403.txt': 0.0, '097279.txt': 0.0, '021797.txt': 0.0, '004492.txt': 0.0, '089818.txt': 0.0, '095324.txt': 0.0, '004933.txt': 0.0, '099706.txt': 0.0, '031074.txt': 0.0, '036666.txt': 0.0, '054233.txt': 0.0, '031140.txt': 0.0, '086163.txt': 0.0, '089577.txt': 0.0, '027503.txt': 0.0, '003681.txt': 0.0, '031784.txt': 0.0, '009309.txt': 0.0, '020114.txt': 0.0, '049599.txt': 0.0, '030557.txt': 0.0, '052265.txt': 0.0, '039860.txt': 0.0, '015260.txt': 0.0, '066861.txt': 0.0, '097970.txt': 0.0, '001127.txt': 0.0, '037341.txt': 0.0, '044549.txt': 0.0, '005430.txt': 0.0, '024022.txt': 0.0, '025422.txt': 0.0}) \n",
            "\n",
            "0.2854413275304792\n",
            "0.0\n",
            "Counter({'008447.txt': 1.0, '043815.txt': 1.0, '001499.txt': 1.0, '068182.txt': 1.0, '078681.txt': 1.0, '070711.txt': 1.0, '046489.txt': 1.0, '000092.txt': 1.0, '057136.txt': 1.0, '079913.txt': 1.0, '092332.txt': 1.0, '069872.txt': 1.0, '099856.txt': 1.0, '077380.txt': 1.0, '022731.txt': 1.0, '070433.txt': 1.0, '076365.txt': 1.0, '098576.txt': 1.0, '056717.txt': 1.0, '092224.txt': 1.0, '004700.txt': 1.0, '014007.txt': 1.0, '003865.txt': 1.0, '005830.txt': 1.0, '096695.txt': 1.0, '016293.txt': 1.0, '084828.txt': 1.0, '003595.txt': 1.0, '041233.txt': 1.0, '052766.txt': 1.0, '053170.txt': 1.0, '091318.txt': 1.0, '055546.txt': 1.0, '086222.txt': 1.0, '013201.txt': 1.0, '080552.txt': 1.0, '048477.txt': 1.0, '078211.txt': 1.0, '031532.txt': 1.0, '000085.txt': 1.0, '052474.txt': 1.0, '092346.txt': 1.0, '076286.txt': 1.0, '065642.txt': 1.0, '026998.txt': 1.0, '061781.txt': 1.0, '003263.txt': 1.0, '059429.txt': 1.0, '034867.txt': 1.0, '060425.txt': 1.0, '024927.txt': 1.0, '023005.txt': 1.0, '024409.txt': 1.0, '061129.txt': 1.0, '082790.txt': 1.0, '078150.txt': 1.0, '025322.txt': 1.0, '093057.txt': 1.0, '061521.txt': 1.0, '069510.txt': 1.0, '066101.txt': 1.0, '062618.txt': 1.0, '077346.txt': 1.0, '073405.txt': 1.0, '069739.txt': 1.0, '096228.txt': 1.0, '089959.txt': 1.0, '014837.txt': 1.0, '032083.txt': 1.0, '091177.txt': 1.0, '049344.txt': 1.0, '006926.txt': 1.0, '006756.txt': 1.0, '036746.txt': 1.0, '055907.txt': 1.0, '068921.txt': 1.0, '031664.txt': 1.0, '016178.txt': 1.0, '023744.txt': 1.0, '096245.txt': 1.0, '094196.txt': 1.0, '012587.txt': 1.0, '087535.txt': 1.0, '017190.txt': 1.0, '029713.txt': 1.0, '050998.txt': 1.0, '033327.txt': 1.0, '056799.txt': 1.0, '084570.txt': 1.0, '099372.txt': 1.0, '024341.txt': 1.0, '087161.txt': 1.0, '025675.txt': 1.0, '070222.txt': 1.0, '039769.txt': 1.0, '045673.txt': 1.0, '035718.txt': 1.0, '012111.txt': 1.0, '074125.txt': 1.0, '010628.txt': 1.0, '051640.txt': 1.0, '022477.txt': 1.0, '081113.txt': 1.0, '081163.txt': 1.0, '022823.txt': 1.0, '040417.txt': 1.0, '057494.txt': 1.0, '010127.txt': 1.0, '070553.txt': 1.0, '006766.txt': 1.0, '025252.txt': 1.0, '012328.txt': 1.0, '091168.txt': 1.0, '073279.txt': 1.0, '078432.txt': 1.0, '041466.txt': 1.0, '033709.txt': 1.0, '093729.txt': 0.8571428571428571, '066194.txt': 0.8461538461538461, '076001.txt': 0.8333333333333334, '026056.txt': 0.8333333333333334, '051820.txt': 0.8333333333333334, '052633.txt': 0.8333333333333334, '006769.txt': 0.8, '006842.txt': 0.8, '002393.txt': 0.8, '055154.txt': 0.7857142857142857, '007667.txt': 0.75, '060404.txt': 0.75, '061177.txt': 0.75, '055858.txt': 0.75, '006307.txt': 0.75, '031383.txt': 0.75, '045251.txt': 0.75, '074927.txt': 0.7142857142857143, '079235.txt': 0.7, '007135.txt': 0.6666666666666666, '014909.txt': 0.6666666666666666, '024648.txt': 0.6666666666666666, '050528.txt': 0.6666666666666666, '060653.txt': 0.6666666666666666, '018383.txt': 0.6666666666666666, '030370.txt': 0.6666666666666666, '042545.txt': 0.6666666666666666, '071619.txt': 0.6666666666666666, '085232.txt': 0.6666666666666666, '005051.txt': 0.6666666666666666, '048696.txt': 0.625, '008653.txt': 0.6, '015420.txt': 0.6, '093286.txt': 0.6, '001510.txt': 0.5714285714285714, '011976.txt': 0.5714285714285714, '058297.txt': 0.5714285714285714, '074696.txt': 0.5714285714285714, '081422.txt': 0.5714285714285714, '099814.txt': 0.5714285714285714, '092127.txt': 0.5714285714285714, '000605.txt': 0.5714285714285714, '078422.txt': 0.5, '046467.txt': 0.5, '042691.txt': 0.5, '069092.txt': 0.5, '096651.txt': 0.5, '079849.txt': 0.5, '018440.txt': 0.5, '043843.txt': 0.5, '056963.txt': 0.5, '073640.txt': 0.5, '000393.txt': 0.5, '083380.txt': 0.5, '094946.txt': 0.5, '048230.txt': 0.5, '045846.txt': 0.5, '079773.txt': 0.5, '030315.txt': 0.5, '077844.txt': 0.5, '097987.txt': 0.5, '007283.txt': 0.5, '097961.txt': 0.4444444444444444, '079208.txt': 0.42857142857142855, '045787.txt': 0.42857142857142855, '080480.txt': 0.4, '001884.txt': 0.4, '018234.txt': 0.4, '040690.txt': 0.4, '068655.txt': 0.4, '025194.txt': 0.4, '065680.txt': 0.375, '064936.txt': 0.375, '063855.txt': 0.375, '098981.txt': 0.375, '031183.txt': 0.3333333333333333, '039087.txt': 0.3333333333333333, '031284.txt': 0.3333333333333333, '039421.txt': 0.3333333333333333, '017391.txt': 0.3333333333333333, '036162.txt': 0.3333333333333333, '064109.txt': 0.3333333333333333, '069632.txt': 0.3333333333333333, '024224.txt': 0.3333333333333333, '056342.txt': 0.3333333333333333, '098965.txt': 0.3333333333333333, '084981.txt': 0.3333333333333333, '036387.txt': 0.3333333333333333, '082631.txt': 0.3333333333333333, '095429.txt': 0.3333333333333333, '079285.txt': 0.3333333333333333, '037140.txt': 0.3333333333333333, '050437.txt': 0.3333333333333333, '008882.txt': 0.3333333333333333, '031336.txt': 0.3333333333333333, '089065.txt': 0.3333333333333333, '083224.txt': 0.3333333333333333, '050237.txt': 0.3333333333333333, '052170.txt': 0.3, '048137.txt': 0.3, '020951.txt': 0.3, '007876.txt': 0.2857142857142857, '045545.txt': 0.2857142857142857, '039621.txt': 0.2727272727272727, '090592.txt': 0.2727272727272727, '000123.txt': 0.25, '043774.txt': 0.25, '000669.txt': 0.25, '080529.txt': 0.25, '049423.txt': 0.25, '016438.txt': 0.25, '069675.txt': 0.25, '098605.txt': 0.25, '031822.txt': 0.25, '056515.txt': 0.25, '099439.txt': 0.25, '043560.txt': 0.25, '002236.txt': 0.25, '046762.txt': 0.25, '017617.txt': 0.25, '083654.txt': 0.23529411764705882, '050332.txt': 0.23076923076923078, '067658.txt': 0.2222222222222222, '076483.txt': 0.2222222222222222, '046349.txt': 0.2222222222222222, '066278.txt': 0.21052631578947367, '054027.txt': 0.2, '077071.txt': 0.2, '082829.txt': 0.2, '021820.txt': 0.2, '036382.txt': 0.2, '002669.txt': 0.2, '005113.txt': 0.2, '049905.txt': 0.2, '028684.txt': 0.2, '067620.txt': 0.2, '089515.txt': 0.2, '086477.txt': 0.2, '066012.txt': 0.2, '081803.txt': 0.18181818181818182, '035918.txt': 0.18181818181818182, '035213.txt': 0.17647058823529413, '073494.txt': 0.16666666666666666, '005862.txt': 0.16666666666666666, '057595.txt': 0.16666666666666666, '012796.txt': 0.16666666666666666, '004066.txt': 0.16666666666666666, '024269.txt': 0.16666666666666666, '020165.txt': 0.16666666666666666, '095431.txt': 0.15384615384615385, '037809.txt': 0.14285714285714285, '048376.txt': 0.14285714285714285, '033096.txt': 0.14285714285714285, '002770.txt': 0.14285714285714285, '021995.txt': 0.14285714285714285, '058993.txt': 0.14285714285714285, '088234.txt': 0.14285714285714285, '014510.txt': 0.14285714285714285, '045061.txt': 0.14285714285714285, '058990.txt': 0.14285714285714285, '056002.txt': 0.14285714285714285, '058009.txt': 0.13333333333333333, '038373.txt': 0.13333333333333333, '075527.txt': 0.125, '006909.txt': 0.125, '064116.txt': 0.125, '010822.txt': 0.125, '067125.txt': 0.125, '076003.txt': 0.125, '021601.txt': 0.125, '053848.txt': 0.1111111111111111, '085841.txt': 0.1111111111111111, '069868.txt': 0.1111111111111111, '015055.txt': 0.1111111111111111, '006304.txt': 0.1111111111111111, '015997.txt': 0.1, '029207.txt': 0.1, '069049.txt': 0.1, '099064.txt': 0.09523809523809523, '000418.txt': 0.09090909090909091, '003516.txt': 0.09090909090909091, '031853.txt': 0.09090909090909091, '076011.txt': 0.09090909090909091, '073000.txt': 0.09090909090909091, '046955.txt': 0.09090909090909091, '004541.txt': 0.08333333333333333, '087285.txt': 0.08333333333333333, '060686.txt': 0.07692307692307693, '062394.txt': 0.07692307692307693, '015020.txt': 0.07142857142857142, '027316.txt': 0.07142857142857142, '011121.txt': 0.058823529411764705, '016020.txt': 0.058823529411764705, '093016.txt': 0.047619047619047616, '022093.txt': 0.043478260869565216, '067501.txt': 0.0, '007627.txt': 0.0, '030394.txt': 0.0, '006704.txt': 0.0, '004108.txt': 0.0, '040560.txt': 0.0, '037117.txt': 0.0, '016618.txt': 0.0, '088711.txt': 0.0, '050190.txt': 0.0, '060807.txt': 0.0, '046017.txt': 0.0, '071236.txt': 0.0, '008771.txt': 0.0, '087599.txt': 0.0, '009186.txt': 0.0, '038947.txt': 0.0, '015031.txt': 0.0, '074582.txt': 0.0, '089555.txt': 0.0, '015966.txt': 0.0, '083086.txt': 0.0, '076990.txt': 0.0, '024157.txt': 0.0, '024481.txt': 0.0, '098226.txt': 0.0, '069976.txt': 0.0, '029645.txt': 0.0, '068503.txt': 0.0, '015327.txt': 0.0, '070873.txt': 0.0, '055208.txt': 0.0, '005409.txt': 0.0, '007137.txt': 0.0, '061651.txt': 0.0, '081919.txt': 0.0, '040239.txt': 0.0, '062436.txt': 0.0, '079028.txt': 0.0, '076491.txt': 0.0, '046080.txt': 0.0, '026213.txt': 0.0, '096028.txt': 0.0, '099898.txt': 0.0, '015161.txt': 0.0, '011450.txt': 0.0, '055693.txt': 0.0, '037710.txt': 0.0, '060780.txt': 0.0, '000447.txt': 0.0, '017471.txt': 0.0, '046478.txt': 0.0, '031641.txt': 0.0, '097277.txt': 0.0, '057424.txt': 0.0, '056962.txt': 0.0, '022931.txt': 0.0, '080737.txt': 0.0, '022361.txt': 0.0, '048500.txt': 0.0, '056909.txt': 0.0, '002760.txt': 0.0, '012713.txt': 0.0, '014411.txt': 0.0, '040801.txt': 0.0, '062926.txt': 0.0, '027864.txt': 0.0, '098780.txt': 0.0, '003218.txt': 0.0, '081689.txt': 0.0, '042705.txt': 0.0, '087773.txt': 0.0, '078779.txt': 0.0, '016058.txt': 0.0, '037440.txt': 0.0, '083342.txt': 0.0, '037853.txt': 0.0, '099187.txt': 0.0, '079672.txt': 0.0, '064402.txt': 0.0, '020453.txt': 0.0, '076969.txt': 0.0, '023818.txt': 0.0, '046310.txt': 0.0, '048771.txt': 0.0, '052909.txt': 0.0, '079200.txt': 0.0, '039086.txt': 0.0, '043523.txt': 0.0, '098080.txt': 0.0, '028359.txt': 0.0, '097926.txt': 0.0, '072112.txt': 0.0, '086250.txt': 0.0, '071437.txt': 0.0, '036579.txt': 0.0, '090936.txt': 0.0, '099551.txt': 0.0, '096679.txt': 0.0, '072292.txt': 0.0, '028914.txt': 0.0, '037011.txt': 0.0, '069228.txt': 0.0, '038962.txt': 0.0, '011447.txt': 0.0, '051749.txt': 0.0, '005628.txt': 0.0, '020351.txt': 0.0, '019821.txt': 0.0, '072315.txt': 0.0, '093206.txt': 0.0, '084496.txt': 0.0, '035016.txt': 0.0, '080908.txt': 0.0, '057543.txt': 0.0, '035550.txt': 0.0, '071899.txt': 0.0, '087884.txt': 0.0, '034622.txt': 0.0, '026686.txt': 0.0, '041961.txt': 0.0, '060931.txt': 0.0, '073337.txt': 0.0, '046266.txt': 0.0, '019591.txt': 0.0, '017738.txt': 0.0, '099124.txt': 0.0, '076138.txt': 0.0, '023554.txt': 0.0, '097067.txt': 0.0, '065028.txt': 0.0, '055968.txt': 0.0, '008173.txt': 0.0, '073660.txt': 0.0, '042740.txt': 0.0, '013611.txt': 0.0, '081675.txt': 0.0, '002314.txt': 0.0, '089448.txt': 0.0, '052702.txt': 0.0, '071181.txt': 0.0, '061878.txt': 0.0, '072069.txt': 0.0, '056205.txt': 0.0, '002449.txt': 0.0, '076685.txt': 0.0, '065127.txt': 0.0, '065307.txt': 0.0, '065387.txt': 0.0, '063210.txt': 0.0, '073526.txt': 0.0, '086570.txt': 0.0, '044675.txt': 0.0, '048888.txt': 0.0, '024374.txt': 0.0, '085278.txt': 0.0, '065832.txt': 0.0, '020040.txt': 0.0, '052609.txt': 0.0, '094410.txt': 0.0, '019275.txt': 0.0, '074753.txt': 0.0, '011086.txt': 0.0, '073784.txt': 0.0, '010360.txt': 0.0, '043608.txt': 0.0, '013019.txt': 0.0, '005978.txt': 0.0, '017699.txt': 0.0, '052846.txt': 0.0, '030546.txt': 0.0, '016165.txt': 0.0, '050729.txt': 0.0, '033703.txt': 0.0, '002648.txt': 0.0, '040011.txt': 0.0, '004471.txt': 0.0, '092144.txt': 0.0, '074104.txt': 0.0, '080135.txt': 0.0, '085920.txt': 0.0, '066880.txt': 0.0, '072166.txt': 0.0, '007365.txt': 0.0, '078583.txt': 0.0, '082393.txt': 0.0, '049750.txt': 0.0, '076684.txt': 0.0, '002656.txt': 0.0, '057990.txt': 0.0, '076640.txt': 0.0, '088208.txt': 0.0, '074264.txt': 0.0, '055104.txt': 0.0, '032353.txt': 0.0, '083595.txt': 0.0, '000031.txt': 0.0, '089050.txt': 0.0, '086288.txt': 0.0, '050119.txt': 0.0, '080304.txt': 0.0, '067346.txt': 0.0, '030050.txt': 0.0, '069178.txt': 0.0, '039187.txt': 0.0, '052168.txt': 0.0, '024179.txt': 0.0, '056324.txt': 0.0, '083142.txt': 0.0, '017025.txt': 0.0, '092690.txt': 0.0, '085279.txt': 0.0, '085782.txt': 0.0, '041293.txt': 0.0, '093797.txt': 0.0, '024429.txt': 0.0, '066955.txt': 0.0, '034805.txt': 0.0, '093214.txt': 0.0, '096966.txt': 0.0, '003915.txt': 0.0, '089811.txt': 0.0, '055216.txt': 0.0, '033823.txt': 0.0, '089304.txt': 0.0, '060132.txt': 0.0, '021818.txt': 0.0, '059383.txt': 0.0, '099508.txt': 0.0, '089468.txt': 0.0, '011346.txt': 0.0, '005267.txt': 0.0, '039301.txt': 0.0, '095993.txt': 0.0, '010024.txt': 0.0, '006922.txt': 0.0, '007286.txt': 0.0, '026185.txt': 0.0, '067329.txt': 0.0, '001947.txt': 0.0, '035994.txt': 0.0, '032248.txt': 0.0, '074245.txt': 0.0, '068739.txt': 0.0, '085737.txt': 0.0, '023731.txt': 0.0, '081491.txt': 0.0, '052454.txt': 0.0, '030675.txt': 0.0, '069213.txt': 0.0, '051097.txt': 0.0, '013029.txt': 0.0, '043298.txt': 0.0, '037221.txt': 0.0, '041390.txt': 0.0, '057191.txt': 0.0, '050666.txt': 0.0, '007698.txt': 0.0, '027247.txt': 0.0, '028416.txt': 0.0, '049261.txt': 0.0, '022160.txt': 0.0, '098353.txt': 0.0, '087257.txt': 0.0, '042957.txt': 0.0, '039772.txt': 0.0, '061196.txt': 0.0, '075817.txt': 0.0, '016748.txt': 0.0, '021652.txt': 0.0, '045104.txt': 0.0, '006113.txt': 0.0, '074300.txt': 0.0, '036157.txt': 0.0, '047695.txt': 0.0, '049802.txt': 0.0, '097104.txt': 0.0, '002716.txt': 0.0, '080457.txt': 0.0, '003992.txt': 0.0, '050382.txt': 0.0, '076477.txt': 0.0, '049784.txt': 0.0, '023107.txt': 0.0, '017701.txt': 0.0, '074679.txt': 0.0, '001153.txt': 0.0, '022534.txt': 0.0, '031872.txt': 0.0, '032116.txt': 0.0, '078963.txt': 0.0, '088672.txt': 0.0, '074006.txt': 0.0, '094198.txt': 0.0, '085192.txt': 0.0, '042761.txt': 0.0, '061668.txt': 0.0, '016253.txt': 0.0, '094548.txt': 0.0, '033796.txt': 0.0, '090433.txt': 0.0, '065299.txt': 0.0, '050486.txt': 0.0, '005768.txt': 0.0, '087656.txt': 0.0, '020090.txt': 0.0, '024403.txt': 0.0, '097279.txt': 0.0, '021797.txt': 0.0, '004492.txt': 0.0, '089818.txt': 0.0, '095324.txt': 0.0, '004933.txt': 0.0, '099706.txt': 0.0, '031074.txt': 0.0, '036666.txt': 0.0, '054233.txt': 0.0, '031140.txt': 0.0, '086163.txt': 0.0, '089577.txt': 0.0, '027503.txt': 0.0, '003681.txt': 0.0, '031784.txt': 0.0, '009309.txt': 0.0, '020114.txt': 0.0, '049599.txt': 0.0, '030557.txt': 0.0, '052265.txt': 0.0, '039860.txt': 0.0, '015260.txt': 0.0, '066861.txt': 0.0, '097970.txt': 0.0, '001127.txt': 0.0, '037341.txt': 0.0, '044549.txt': 0.0, '005430.txt': 0.0, '024022.txt': 0.0, '025422.txt': 0.0})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST: result final  '066194.txt': 6\n",
        "# '066194.txt': [\"081206.txt\", \"026970.txt\", \"077831.txt\", \"051354.txt\", \"050927.txt\", \"030530.txt\", \"002428.txt\", \"086928.txt\", \"007418.txt\", \"092405.txt\", \"034038.txt\", \"094249.txt\", \"051831.txt\"]\n",
        "\n",
        "#doc_to_top_k_raw[\"038025.txt\"]\n",
        "\n",
        "result = Counter()\n",
        "try:\n",
        "  for ref in doc_to_refs[\"066194.txt\"]: \n",
        "    print(ref)\n",
        "    query_dominant = doc_to_dominant_topic[\"066194.txt\"] #94.0\n",
        "    print(query_dominant)\n",
        "    top_k_from_ref = doc_to_top_k_raw[ref].replace(\"[\",\"\").replace(\"]\",\"\").split(' ')\n",
        "    print(top_k_from_ref)\n",
        "    if (convert_float_to_int_string(query_dominant) in top_k_from_ref):\n",
        "      result[\"066194.txt\"] = result[\"066194.txt\"] + 1\n",
        "      print(result)\n",
        "except:\n",
        "    result[\"066194.txt\"] = result[\"066194.txt\"] + 0\n",
        "    print(result)\n",
        "\n",
        "result[\"066194.txt\"] = result[\"066194.txt\"] / len(doc_to_refs[\"066194.txt\"])\n",
        "\n",
        "\n",
        "\n",
        "print(len(doc_to_refs[\"066194.txt\"]))\n",
        "print(result)\n",
        "\n",
        "print(6/13)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcEdCEq1_oDX",
        "outputId": "f937a35d-e27d-4118-ea20-6e9cd0c69397"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "081206.txt\n",
            "21.0\n",
            "['21', '54', '92', '85', '68', '9', '22', '43', '78', '2']\n",
            "Counter({'066194.txt': 1})\n",
            "026970.txt\n",
            "21.0\n",
            "['21', '54', '92', '85', '68', '9', '22', '43', '78', '2']\n",
            "Counter({'066194.txt': 2})\n",
            "077831.txt\n",
            "21.0\n",
            "['21', '54', '92', '85', '68', '9', '22', '43', '78', '2']\n",
            "Counter({'066194.txt': 3})\n",
            "051354.txt\n",
            "21.0\n",
            "['21', '54', '92', '85', '68', '9', '22', '43', '78', '2']\n",
            "Counter({'066194.txt': 4})\n",
            "050927.txt\n",
            "21.0\n",
            "['21', '54', '92', '85', '68', '9', '22', '43', '78', '2']\n",
            "Counter({'066194.txt': 5})\n",
            "030530.txt\n",
            "21.0\n",
            "['21', '54', '92', '85', '68', '9', '22', '43', '78', '2']\n",
            "Counter({'066194.txt': 6})\n",
            "002428.txt\n",
            "21.0\n",
            "Counter({'066194.txt': 6})\n",
            "13\n",
            "Counter({'066194.txt': 0.46153846153846156})\n",
            "0.46153846153846156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc_to_top_k_raw[\"081206.txt\"])\n",
        "print(doc_to_top_k_raw[\"026970.txt\"])\n",
        "print(doc_to_top_k_raw[\"066194.txt\"])\n",
        "\n",
        "#print(doc_to_dominant_topic['031183.txt'])\n",
        "#print(doc_to_top_k['050912.txt'])\n",
        "#print(doc_to_top_k['051897.txt'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN3a8aDlHBCh",
        "outputId": "2ccdc1b1-7998-4c21-903c-a53b7f319f88"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[21 54 92 85 68 9 22 43 78 2]\n",
            "[21 54 92 85 68 9 22 43 78 2]\n",
            "[21 54 92 85 68 9 22 43 78 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main code evaluation, with threshold and K variable \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e4txd-NYe4H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate precision considering top K dominant topics \n",
        "\n",
        "#Vary K from 5 to 115, adding 5\n",
        "k = list(range(5,116,5))\n",
        "\n",
        "#load docs \n",
        "bert_result_file = \"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/results/bertopic_documents_final-k-max.csv\"\n",
        "document_references_file = \"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/task1_train_labels.json\"\n",
        "\n",
        "# Compute the maps for analysis\n",
        "doc_to_dominant_topic = generate_dominant_map(bert_result_file) #dict: doc -> dominant topic ; 4415 items\n",
        "doc_to_refs = generate_document_map(document_references_file) #dict: query -> answers ; 650 items (queries)\n",
        "\n",
        "#Add a threshold\n",
        "threshold = 0.4\n",
        "\n",
        "#generate many maps \n",
        "top_k = []\n",
        "for i in k:\n",
        "  #top_k.append(generate_top_k_map(bert_result_file,i)) # no threshould considered\n",
        "  top_k.append(generate_top_k_map_threshold(bert_result_file,i,threshold))\n",
        "\n",
        "answers = []\n",
        "for key in top_k:\n",
        " try: \n",
        "  recall_k = dominant_inside_top_k(doc_to_dominant_topic, key, doc_to_refs)\n",
        "  avg_recall = statistics.mean(recall_k.values())\n",
        "  answers.append(avg_recall)\n",
        " except:\n",
        "  continue\n",
        "\n",
        "#print(top_k)\n",
        "\n",
        "\n",
        "\n",
        "# to convert lists to dictionary\n",
        "#d = dict(zip(k, answers))\n"
      ],
      "metadata": {
        "id": "g9G8MQCclakf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for number in answers:\n",
        "  print(\"{:10.4f}\".format(number)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLT4FFY_Xj_G",
        "outputId": "48b2acea-0956-4e45-8731-69b8fd1d95f1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0.6316\n",
            "    0.7027\n",
            "    0.7404\n",
            "    0.7713\n",
            "    0.7953\n",
            "    0.8233\n",
            "    0.8533\n",
            "    0.8796\n",
            "    0.9058\n",
            "    0.9246\n",
            "    0.9428\n",
            "    0.9510\n",
            "    0.9570\n",
            "    0.9627\n",
            "    0.9692\n",
            "    0.9748\n",
            "    0.9823\n",
            "    0.9845\n",
            "    0.9886\n",
            "    0.9911\n",
            "    0.9933\n",
            "    0.9960\n",
            "    0.9984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot recall vs K\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "\n",
        "#k.insert(0, '1')\n",
        "x = k\n",
        "#answers.insert(0,'0.46')\n",
        "y=answers\n",
        "\n",
        "plt.xlabel(\"K*5\")\n",
        "plt.ylabel(\"Recall\")\n",
        "plt.title('threshold = 0.2')\n",
        "plt.plot(y, marker = 'o')\n",
        "plt.grid()\n",
        "plt.show()  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "m_vZ8mLhw9zS",
        "outputId": "a24c01be-dfdf-43e6-f1e0-830d52eed18d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWg9znKOQ9r0"
      },
      "source": [
        "## Retrieval function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htl8qIkMZ8h1",
        "outputId": "7f46bd89-fcad-42ff-fc45-4588c08c6485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Acesso ao drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def read_list_of_ids(file_path: str):\n",
        "    with open(file_path) as infile:\n",
        "        id_lists = infile.read().splitlines()\n",
        "    return id_lists\n",
        "\n",
        "\n",
        "# Input files\n",
        "bert_result_file = \"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/results/bertopic_documents-final.csv\"\n",
        "document_references_file = \"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/task1_train_labels.json\"\n",
        "embeddings_bin = np.load(\"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/embedding-vector.npy\")\n",
        "embeddings_list = embeddings_bin.tolist()\n",
        "list_of_ids = read_list_of_ids(\"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/list_of_ids\")\n",
        "\n",
        "\n",
        "# print(embeddings_list)\n"
      ],
      "metadata": {
        "id": "bU-fTeL1g3Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = len(queries_to_refs['008447.txt'])\n",
        "print(size)"
      ],
      "metadata": {
        "id": "dUOhmhDFfG_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dominant_map(csv_filepath: str) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    return dict(zip(df.Document_id, df.Dominant_Topic))\n",
        "\n",
        "def generate_ids_to_embeddings(csv_filepath: str) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\";\")\n",
        "    return dict(zip(df.Topic, df.Embedding))\n",
        "\n",
        "def generate_top_k_map(csv_filepath: str, k: int) -> map:\n",
        "    df = pd.read_csv(csv_filepath, sep=\",\")\n",
        "    result_map = dict(zip(df.Document_id, df.Top5_Topics)) # TOP 5\n",
        "    # print(\"Pre-filtering map:\", result_map)\n",
        "    filtered_map = remove_all_elements_greater_than_k(result_map, k)\n",
        "    # print(\"Filtered on top K map:\", filtered_map)\n",
        "    return filtered_map\n",
        "\n",
        "\n",
        "def remove_all_elements_greater_than_k(input_map : map, cut_value: int):\n",
        "    new_map = dict()\n",
        "    for (key, v) in input_map.items():\n",
        "        l = str_to_list(v)\n",
        "        new_map[key] = build_str(l, cut_value)\n",
        "    return new_map\n",
        "\n",
        "\n",
        "def str_to_list(value: str) -> list:\n",
        "    cleaned_v = value.replace(\"]\", \"\").replace(\"[\", \"\").strip()\n",
        "    return re.split(\"\\s+\", cleaned_v)\n",
        "\n",
        "\n",
        "def generate_document_map(json_file: str) -> map:\n",
        "    with open(json_file) as json_data:\n",
        "        data = json.load(json_data)\n",
        "        return data\n",
        "\n",
        "def build_str(l: list, cut_value: int) -> str:\n",
        "   return \"[\" + \" \".join(l[0:cut_value]) + \"]\"\n",
        "\n",
        "\n",
        "def convert_float_to_int_string(value: float) -> str:\n",
        "    return str(int(float(value)))\n",
        "\n",
        "# Compute the maps for analysis: \n",
        "  #Doc -> Dominant topic\n",
        "all_docs_to_dominant_topic = generate_dominant_map(bert_result_file)\n",
        "#print(all_docs_to_dominant_topic) # OK\n",
        "queries_to_refs = generate_document_map(document_references_file)\n",
        "#print(queries_to_refs) # OK\n",
        "\n",
        "# Group docs with the same dominant topic [and define top K] <<<<<\n",
        "all_docs_to_top_k = generate_top_k_map(bert_result_file, 10) #top K = 10\n",
        "#print(all_docs_to_top_k) # OK\n",
        "\n",
        "\n",
        "# Generate list of docs with same dominant topic's group for top K\n",
        "def add_to_inverted_list(inverted_map, doc, topic):\n",
        "  if topic not in inverted_map:\n",
        "    inverted_map[topic] = set()\n",
        "  inverted_map[topic].add(doc)\n",
        "\n",
        "\n",
        "def get_inverted_map(all_docs_to_dominant_topic):\n",
        "  inverted_map = dict()\n",
        "  for (doc_i, topic_i) in all_docs_to_dominant_topic.items():\n",
        "    add_to_inverted_list(inverted_map, doc_i, topic_i)\n",
        "  return inverted_map\n",
        "\n",
        "inverted_map = get_inverted_map(all_docs_to_dominant_topic)\n",
        "\n",
        "def queries_to_docs(queries, inverted_map, all_docs_to_top_k):\n",
        "    query_candidates = dict()\n",
        "    for query in queries:\n",
        "        top_k_list = str_to_list(all_docs_to_top_k[query]) # '017190.txt': '[66 105 19 35]'\n",
        "        query_candidates[query] = set()\n",
        "        for topic in top_k_list:\n",
        "            inv_list = inverted_map[float(topic)]\n",
        "  #          print(\"For dominant:\", topic, \"size: \", len(inv_list))\n",
        "            query_candidates[query] = query_candidates[query].union(inv_list)\n",
        "   #         print(\"Candidates after union with topic:\", topic, \", size: \",len(query_candidates[query]))\n",
        "    #        print(\"Final candidates for topic \", topic, \", size:\", len(query_candidates[query]))\n",
        "    return query_candidates\n",
        "\n",
        "def write_doc_to_embeddings(map: map, filename: str) -> None:\n",
        "  #  print(\"To write at file: \" + \"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/\"+filename)\n",
        "    with open(\"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/\"+filename,'w',encoding = 'utf-8') as f:\n",
        "        f.write(\"query,candidates_embeddings\\n\")\n",
        "        for (k,v) in map.items():\n",
        "            f.write( str(k) + \";\" + str(v) + \"\\n\" )\n",
        "\n",
        "\n",
        "query_candidates = queries_to_docs(queries_to_refs.keys(), inverted_map, all_docs_to_top_k)\n",
        "# print(queryToDocs['008447.txt'])\n",
        "# print(inverted_map)\n",
        "# print(inverted_map[21.0].union(inverted_map[18.0]))\n",
        "# print(len(inverted_map[117.0]))\n",
        "\n",
        "def convert_doc_ids_to_embeddings(query_candidates :map, ids_to_embeddings: map) -> map:\n",
        "    query_embeddings = dict()\n",
        "    for (query, candidates) in query_candidates.items():\n",
        "        query_embeddings[query] = [ids_to_embeddings[candidate] for candidate in candidates]\n",
        "    return query_embeddings\n",
        "\n",
        "\n",
        "\n",
        "ids_to_embeddings = dict(zip(list_of_ids, embeddings_list))\n",
        "# print(ids_to_embeddings)\n",
        "\n",
        "query_to_embeddings = convert_doc_ids_to_embeddings(query_candidates, ids_to_embeddings)\n",
        "# print(query_to_embeddings)\n",
        "#write_doc_to_embeddings(query_to_embeddings, \"query_to_embeddings\")\n",
        "\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# key=query, values=candidates\n",
        "\n",
        "results_raw=dict()\n",
        "results = dict()\n",
        "for key, values in query_to_embeddings.items():\n",
        "  X = np.array(ids_to_embeddings[key]).reshape(1,-1)\n",
        "  Y = values\n",
        "  similarities = cosine_similarity(X, Y)\n",
        "  sim = similarities[0]\n",
        "  lenght = len(queries_to_refs[key])\n",
        "  ind = np.argpartition(sim, lenght)[lenght:]\n",
        "  #ind = np.argpartition(sim, -35)[-35:]   # Pra variar o top N mais similares; pega os N últimos, pq está em ordem crescente;\n",
        "  top = ind[np.argsort(sim[ind])] # Retorna o indice (original) dos valores no Top N\n",
        "  value = list(query_candidates[key])\n",
        "  answers_raw= []\n",
        "  answers = dict()\n",
        "  for idx in top:\n",
        "    if value[idx] != key:\n",
        "      answers_raw.append(value[idx])\n",
        "      answers[value[idx]] = sim[idx]\n",
        "  results[key] = answers\n",
        "  results_raw[key] = answers_raw\n"
      ],
      "metadata": {
        "id": "zI1asFBCdSVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final results evaluation"
      ],
      "metadata": {
        "id": "1wLIoyb7cp2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "#Checking the biggest number of answers\n",
        "print(queries_to_refs,'\\n')\n",
        "length_dict = {key: len(value) for key, value in queries_to_refs.items()}\n",
        "print(length_dict)\n",
        "print(max(length_dict.values()), 'is the maximum number of answers to a query')\n",
        "\n",
        "def write_document_map_to_csv(map: map, filename: str) -> None:\n",
        "    print(\"To write at file: \" + \"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/evaluation/\"+filename)\n",
        "    with open(\"/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/evaluation/\"+filename,'w',encoding = 'utf-8') as f:\n",
        "        f.write(\"document,percentual\\n\")\n",
        "        for (k,v) in map.items():\n",
        "            f.write( str(k) + \",\" + str(v) + \"\\n\" )\n",
        "\n",
        "### Def Precision at N (N = len(results_raw) = 11)\n",
        "def precision_N(results_raw, queries_to_refs) -> map:\n",
        "    precision_results= Counter()\n",
        "    for query,candidates in results_raw.items():\n",
        "      for candidate in results_raw[query][::-1]:\n",
        "        if candidate in queries_to_refs[query]:\n",
        "          precision_results[query] = precision_results[query] +1\n",
        "      precision_results[query] = precision_results[query] / len(results_raw[query])\n",
        "    return precision_results\n",
        "\n",
        "### Def Recall at N (N = len(results_raw))\n",
        "def recall_N(results_raw, queries_to_refs) ->map:\n",
        "  recall_results= Counter()\n",
        "  for query,candidates in results_raw.items():\n",
        "    for candidate in results_raw[query][::-1]:\n",
        "      if candidate in queries_to_refs[query]:\n",
        "        recall_results[query] = recall_results[query] +1\n",
        "    recall_results[query] = recall_results[query] / len(queries_to_refs[query])\n",
        "  return recall_results\n",
        "\n",
        "\n",
        "\n",
        "##Calculate results\n",
        "#Calculate Precision\n",
        "precision_N = precision_N(results_raw, queries_to_refs)\n",
        "print(precision_N)\n",
        "precision_N_mean = statistics.mean(precision_N.values())\n",
        "print(precision_N_mean)\n",
        "#write_document_map_to_csv(precision_N,'Final_precision_N.csv')\n",
        "\n",
        "#Calculate Recall\n",
        "recall_N = recall_N(results_raw, queries_to_refs)\n",
        "#print(recall_N)\n",
        "recall_N_mean = statistics.mean(recall_N.values())\n",
        "print(recall_N_mean)\n",
        "#write_document_map_to_csv(recall_N,'Final_recall_N.csv')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "Aw7GZLRYWmwF",
        "outputId": "6d1c9739-d03c-4a0a-9688-dc7d76c0ad01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b74d8440e41f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Checking the biggest number of answers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries_to_refs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlength_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries_to_refs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'queries_to_refs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_dict = statistics.mean(length_dict.values())\n",
        "print(mean_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxDiv588Fv2c",
        "outputId": "9155e976-aae6-4bdb-997d-457440c7b89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.093846153846154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(recall_N)\n",
        "print(recall_N['008447.txt'])\n",
        "print('candidadates: ',results['008447.txt'])\n",
        "print('gabarito: ', queries_to_refs['008447.txt'][::-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7siMi23WfmI",
        "outputId": "ac27edd2-8624-4866-b01c-de8e00a9d80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'067501.txt': 1.0, '030394.txt': 1.0, '001499.txt': 1.0, '031284.txt': 1.0, '040560.txt': 1.0, '078681.txt': 1.0, '070711.txt': 1.0, '057136.txt': 1.0, '079913.txt': 1.0, '069872.txt': 1.0, '099856.txt': 1.0, '007137.txt': 1.0, '096028.txt': 1.0, '076365.txt': 1.0, '098576.txt': 1.0, '056717.txt': 1.0, '037710.txt': 1.0, '056909.txt': 1.0, '014411.txt': 1.0, '040801.txt': 1.0, '096695.txt': 1.0, '084828.txt': 1.0, '098780.txt': 1.0, '041233.txt': 1.0, '052766.txt': 1.0, '023818.txt': 1.0, '046310.txt': 1.0, '079200.txt': 1.0, '053170.txt': 1.0, '039086.txt': 1.0, '086250.txt': 1.0, '048477.txt': 1.0, '031532.txt': 1.0, '072292.txt': 1.0, '092346.txt': 1.0, '076286.txt': 1.0, '061781.txt': 1.0, '084496.txt': 1.0, '026686.txt': 1.0, '024927.txt': 1.0, '023005.txt': 1.0, '024409.txt': 1.0, '061129.txt': 1.0, '082829.txt': 1.0, '061177.txt': 1.0, '078150.txt': 1.0, '042740.txt': 1.0, '093057.txt': 1.0, '061521.txt': 1.0, '066101.txt': 1.0, '062618.txt': 1.0, '072069.txt': 1.0, '077346.txt': 1.0, '073405.txt': 1.0, '065307.txt': 1.0, '063210.txt': 1.0, '089959.txt': 1.0, '085278.txt': 1.0, '094410.txt': 1.0, '049344.txt': 1.0, '013019.txt': 1.0, '074264.txt': 1.0, '029713.txt': 1.0, '030050.txt': 1.0, '033327.txt': 1.0, '099439.txt': 1.0, '099372.txt': 1.0, '024341.txt': 1.0, '099508.txt': 1.0, '025675.txt': 1.0, '070222.txt': 1.0, '023731.txt': 1.0, '074125.txt': 1.0, '010628.txt': 1.0, '021652.txt': 1.0, '049802.txt': 1.0, '032116.txt': 1.0, '006766.txt': 1.0, '095324.txt': 1.0, '054233.txt': 1.0, '012328.txt': 1.0, '020114.txt': 1.0, '073279.txt': 1.0, '015260.txt': 1.0, '078432.txt': 1.0, '079208.txt': 0.8571428571428571, '006926.txt': 0.8571428571428571, '006769.txt': 0.8, '014837.txt': 0.8, '022477.txt': 0.8, '060404.txt': 0.75, '073640.txt': 0.75, '037011.txt': 0.75, '020351.txt': 0.75, '059429.txt': 0.75, '073526.txt': 0.75, '087161.txt': 0.75, '039772.txt': 0.75, '041466.txt': 0.75, '058297.txt': 0.7142857142857143, '039087.txt': 0.6666666666666666, '006704.txt': 0.6666666666666666, '009186.txt': 0.6666666666666666, '070433.txt': 0.6666666666666666, '000085.txt': 0.6666666666666666, '060931.txt': 0.6666666666666666, '034867.txt': 0.6666666666666666, '046266.txt': 0.6666666666666666, '025322.txt': 0.6666666666666666, '017190.txt': 0.6666666666666666, '079285.txt': 0.6666666666666666, '037140.txt': 0.6666666666666666, '031336.txt': 0.6666666666666666, '074679.txt': 0.6666666666666666, '010127.txt': 0.6666666666666666, '085232.txt': 0.6666666666666666, '005051.txt': 0.6666666666666666, '080480.txt': 0.6, '086222.txt': 0.6, '015997.txt': 0.6, '087535.txt': 0.6, '048137.txt': 0.6, '068655.txt': 0.6, '086477.txt': 0.6, '003992.txt': 0.6, '074696.txt': 0.5714285714285714, '047695.txt': 0.5714285714285714, '008447.txt': 0.5, '078422.txt': 0.5, '000123.txt': 0.5, '046467.txt': 0.5, '088711.txt': 0.5, '046489.txt': 0.5, '092332.txt': 0.5, '055208.txt': 0.5, '042691.txt': 0.5, '000669.txt': 0.5, '092224.txt': 0.5, '060780.txt': 0.5, '096651.txt': 0.5, '003865.txt': 0.5, '016293.txt': 0.5, '018440.txt': 0.5, '043843.txt': 0.5, '080552.txt': 0.5, '052474.txt': 0.5, '000393.txt': 0.5, '003263.txt': 0.5, '083380.txt': 0.5, '060425.txt': 0.5, '048230.txt': 0.5, '008173.txt': 0.5, '031822.txt': 0.5, '086570.txt': 0.5, '032083.txt': 0.5, '051820.txt': 0.5, '011086.txt': 0.5, '036746.txt': 0.5, '016178.txt': 0.5, '018383.txt': 0.5, '056799.txt': 0.5, '084570.txt': 0.5, '093214.txt': 0.5, '006307.txt': 0.5, '081491.txt': 0.5, '030675.txt': 0.5, '006113.txt': 0.5, '042761.txt': 0.5, '021797.txt': 0.5, '004933.txt': 0.5, '027503.txt': 0.5, '007876.txt': 0.42857142857142855, '000092.txt': 0.42857142857142855, '015020.txt': 0.42857142857142855, '058993.txt': 0.42857142857142855, '000605.txt': 0.42857142857142855, '066861.txt': 0.42857142857142855, '008653.txt': 0.4, '046017.txt': 0.4, '054027.txt': 0.4, '069049.txt': 0.4, '074753.txt': 0.4, '006756.txt': 0.4, '003915.txt': 0.4, '067620.txt': 0.4, '074006.txt': 0.4, '066194.txt': 0.38461538461538464, '050332.txt': 0.38461538461538464, '075527.txt': 0.375, '006909.txt': 0.375, '064116.txt': 0.375, '063855.txt': 0.375, '087257.txt': 0.375, '031383.txt': 0.375, '067329.txt': 0.36363636363636365, '016020.txt': 0.35294117647058826, '053848.txt': 0.3333333333333333, '076001.txt': 0.3333333333333333, '068182.txt': 0.3333333333333333, '077380.txt': 0.3333333333333333, '029645.txt': 0.3333333333333333, '014909.txt': 0.3333333333333333, '017391.txt': 0.3333333333333333, '004700.txt': 0.3333333333333333, '064109.txt': 0.3333333333333333, '080737.txt': 0.3333333333333333, '012713.txt': 0.3333333333333333, '078779.txt': 0.3333333333333333, '005862.txt': 0.3333333333333333, '020453.txt': 0.3333333333333333, '050528.txt': 0.3333333333333333, '051749.txt': 0.3333333333333333, '026998.txt': 0.3333333333333333, '057543.txt': 0.3333333333333333, '035550.txt': 0.3333333333333333, '082790.txt': 0.3333333333333333, '056342.txt': 0.3333333333333333, '098965.txt': 0.3333333333333333, '036387.txt': 0.3333333333333333, '068921.txt': 0.3333333333333333, '002656.txt': 0.3333333333333333, '083595.txt': 0.3333333333333333, '000031.txt': 0.3333333333333333, '096245.txt': 0.3333333333333333, '050437.txt': 0.3333333333333333, '039769.txt': 0.3333333333333333, '015055.txt': 0.3333333333333333, '008882.txt': 0.3333333333333333, '022823.txt': 0.3333333333333333, '071619.txt': 0.3333333333333333, '049784.txt': 0.3333333333333333, '085192.txt': 0.3333333333333333, '065299.txt': 0.3333333333333333, '079235.txt': 0.3, '093016.txt': 0.2857142857142857, '024481.txt': 0.2857142857142857, '045787.txt': 0.2857142857142857, '076969.txt': 0.2857142857142857, '093729.txt': 0.2857142857142857, '080908.txt': 0.2857142857142857, '048376.txt': 0.2857142857142857, '092127.txt': 0.2857142857142857, '088234.txt': 0.2857142857142857, '051097.txt': 0.2857142857142857, '003516.txt': 0.2727272727272727, '043774.txt': 0.25, '050190.txt': 0.25, '007667.txt': 0.25, '068503.txt': 0.25, '080529.txt': 0.25, '042705.txt': 0.25, '049423.txt': 0.25, '016058.txt': 0.25, '097926.txt': 0.25, '090936.txt': 0.25, '098605.txt': 0.25, '048696.txt': 0.25, '002314.txt': 0.25, '089448.txt': 0.25, '005978.txt': 0.25, '089050.txt': 0.25, '050998.txt': 0.25, '069178.txt': 0.25, '043560.txt': 0.25, '068739.txt': 0.25, '085737.txt': 0.25, '002236.txt': 0.25, '098353.txt': 0.25, '017617.txt': 0.25, '037341.txt': 0.25, '039421.txt': 0.2222222222222222, '069632.txt': 0.2222222222222222, '081113.txt': 0.2222222222222222, '040417.txt': 0.2222222222222222, '076483.txt': 0.2222222222222222, '043815.txt': 0.2, '074582.txt': 0.2, '015420.txt': 0.2, '036579.txt': 0.2, '038962.txt': 0.2, '052170.txt': 0.2, '077071.txt': 0.2, '029207.txt': 0.2, '013611.txt': 0.2, '094196.txt': 0.2, '050119.txt': 0.2, '012587.txt': 0.2, '020951.txt': 0.2, '093286.txt': 0.2, '089304.txt': 0.2, '049905.txt': 0.2, '028684.txt': 0.2, '040690.txt': 0.2, '002393.txt': 0.2, '080457.txt': 0.2, '066012.txt': 0.2, '061668.txt': 0.2, '025194.txt': 0.2, '031784.txt': 0.2, '000418.txt': 0.18181818181818182, '052846.txt': 0.18181818181818182, '035918.txt': 0.18181818181818182, '073000.txt': 0.18181818181818182, '046955.txt': 0.18181818181818182, '090592.txt': 0.18181818181818182, '036666.txt': 0.18181818181818182, '069976.txt': 0.16666666666666666, '024224.txt': 0.16666666666666666, '026056.txt': 0.16666666666666666, '076138.txt': 0.16666666666666666, '044675.txt': 0.16666666666666666, '073784.txt': 0.16666666666666666, '095429.txt': 0.16666666666666666, '051640.txt': 0.16666666666666666, '097277.txt': 0.16, '066278.txt': 0.15789473684210525, '062394.txt': 0.15384615384615385, '015966.txt': 0.14285714285714285, '017471.txt': 0.14285714285714285, '037809.txt': 0.14285714285714285, '035016.txt': 0.14285714285714285, '019275.txt': 0.14285714285714285, '081422.txt': 0.14285714285714285, '074245.txt': 0.14285714285714285, '014510.txt': 0.14285714285714285, '045061.txt': 0.14285714285714285, '045545.txt': 0.14285714285714285, '058990.txt': 0.14285714285714285, '072112.txt': 0.13333333333333333, '005113.txt': 0.13333333333333333, '038373.txt': 0.13333333333333333, '098226.txt': 0.125, '065680.txt': 0.125, '069228.txt': 0.125, '065028.txt': 0.125, '067125.txt': 0.125, '056515.txt': 0.125, '092144.txt': 0.125, '079773.txt': 0.125, '076477.txt': 0.125, '035213.txt': 0.11764705882352941, '083654.txt': 0.11764705882352941, '008771.txt': 0.1111111111111111, '020165.txt': 0.1111111111111111, '039301.txt': 0.1111111111111111, '069213.txt': 0.1111111111111111, '067658.txt': 0.1111111111111111, '006304.txt': 0.1111111111111111, '046349.txt': 0.1111111111111111, '060807.txt': 0.1, '081675.txt': 0.1, '041293.txt': 0.1, '024022.txt': 0.1, '039621.txt': 0.09090909090909091, '061878.txt': 0.09090909090909091, '081803.txt': 0.09090909090909091, '075817.txt': 0.09090909090909091, '099187.txt': 0.08823529411764706, '016618.txt': 0.08333333333333333, '004541.txt': 0.08333333333333333, '027864.txt': 0.08333333333333333, '004066.txt': 0.08333333333333333, '017699.txt': 0.08333333333333333, '011346.txt': 0.08333333333333333, '060686.txt': 0.07692307692307693, '015161.txt': 0.07692307692307693, '028359.txt': 0.07692307692307693, '076491.txt': 0.07142857142857142, '055154.txt': 0.07142857142857142, '058009.txt': 0.06666666666666667, '062436.txt': 0.05555555555555555, '099064.txt': 0.047619047619047616, '022093.txt': 0.043478260869565216, '007627.txt': 0.0, '031183.txt': 0.0, '004108.txt': 0.0, '037117.txt': 0.0, '071236.txt': 0.0, '087599.txt': 0.0, '038947.txt': 0.0, '015031.txt': 0.0, '089555.txt': 0.0, '083086.txt': 0.0, '007135.txt': 0.0, '076990.txt': 0.0, '024157.txt': 0.0, '085841.txt': 0.0, '001510.txt': 0.0, '015327.txt': 0.0, '022731.txt': 0.0, '070873.txt': 0.0, '024648.txt': 0.0, '005409.txt': 0.0, '061651.txt': 0.0, '081919.txt': 0.0, '040239.txt': 0.0, '087285.txt': 0.0, '079028.txt': 0.0, '046080.txt': 0.0, '026213.txt': 0.0, '099898.txt': 0.0, '069092.txt': 0.0, '011450.txt': 0.0, '055693.txt': 0.0, '036162.txt': 0.0, '000447.txt': 0.0, '046478.txt': 0.0, '031641.txt': 0.0, '057424.txt': 0.0, '056962.txt': 0.0, '022931.txt': 0.0, '014007.txt': 0.0, '005830.txt': 0.0, '022361.txt': 0.0, '001884.txt': 0.0, '048500.txt': 0.0, '095431.txt': 0.0, '002760.txt': 0.0, '079849.txt': 0.0, '062926.txt': 0.0, '003218.txt': 0.0, '081689.txt': 0.0, '087773.txt': 0.0, '003595.txt': 0.0, '037440.txt': 0.0, '083342.txt': 0.0, '037853.txt': 0.0, '064936.txt': 0.0, '079672.txt': 0.0, '073494.txt': 0.0, '064402.txt': 0.0, '048771.txt': 0.0, '052909.txt': 0.0, '091318.txt': 0.0, '055546.txt': 0.0, '011121.txt': 0.0, '043523.txt': 0.0, '098080.txt': 0.0, '057595.txt': 0.0, '013201.txt': 0.0, '071437.txt': 0.0, '056963.txt': 0.0, '078211.txt': 0.0, '097961.txt': 0.0, '016438.txt': 0.0, '069675.txt': 0.0, '099551.txt': 0.0, '096679.txt': 0.0, '028914.txt': 0.0, '011447.txt': 0.0, '005628.txt': 0.0, '060653.txt': 0.0, '019821.txt': 0.0, '065642.txt': 0.0, '012796.txt': 0.0, '072315.txt': 0.0, '010822.txt': 0.0, '093206.txt': 0.0, '011976.txt': 0.0, '071899.txt': 0.0, '087884.txt': 0.0, '034622.txt': 0.0, '041961.txt': 0.0, '094946.txt': 0.0, '073337.txt': 0.0, '069868.txt': 0.0, '019591.txt': 0.0, '017738.txt': 0.0, '099124.txt': 0.0, '023554.txt': 0.0, '097067.txt': 0.0, '055968.txt': 0.0, '073660.txt': 0.0, '069510.txt': 0.0, '052702.txt': 0.0, '071181.txt': 0.0, '021820.txt': 0.0, '056205.txt': 0.0, '002449.txt': 0.0, '076685.txt': 0.0, '069739.txt': 0.0, '065127.txt': 0.0, '065387.txt': 0.0, '006842.txt': 0.0, '096228.txt': 0.0, '048888.txt': 0.0, '024374.txt': 0.0, '033096.txt': 0.0, '084981.txt': 0.0, '065832.txt': 0.0, '020040.txt': 0.0, '036382.txt': 0.0, '052609.txt': 0.0, '091177.txt': 0.0, '076003.txt': 0.0, '010360.txt': 0.0, '043608.txt': 0.0, '024269.txt': 0.0, '030546.txt': 0.0, '016165.txt': 0.0, '050729.txt': 0.0, '033703.txt': 0.0, '002648.txt': 0.0, '055907.txt': 0.0, '045846.txt': 0.0, '040011.txt': 0.0, '004471.txt': 0.0, '082631.txt': 0.0, '074104.txt': 0.0, '080135.txt': 0.0, '085920.txt': 0.0, '066880.txt': 0.0, '072166.txt': 0.0, '007365.txt': 0.0, '031664.txt': 0.0, '002770.txt': 0.0, '078583.txt': 0.0, '082393.txt': 0.0, '049750.txt': 0.0, '076684.txt': 0.0, '031853.txt': 0.0, '057990.txt': 0.0, '076640.txt': 0.0, '021995.txt': 0.0, '088208.txt': 0.0, '002669.txt': 0.0, '055104.txt': 0.0, '023744.txt': 0.0, '032353.txt': 0.0, '086288.txt': 0.0, '080304.txt': 0.0, '067346.txt': 0.0, '018234.txt': 0.0, '030315.txt': 0.0, '039187.txt': 0.0, '030370.txt': 0.0, '052168.txt': 0.0, '024179.txt': 0.0, '056324.txt': 0.0, '083142.txt': 0.0, '017025.txt': 0.0, '092690.txt': 0.0, '085279.txt': 0.0, '027316.txt': 0.0, '085782.txt': 0.0, '076011.txt': 0.0, '099814.txt': 0.0, '042545.txt': 0.0, '093797.txt': 0.0, '052633.txt': 0.0, '024429.txt': 0.0, '066955.txt': 0.0, '074927.txt': 0.0, '034805.txt': 0.0, '096966.txt': 0.0, '089811.txt': 0.0, '055216.txt': 0.0, '033823.txt': 0.0, '060132.txt': 0.0, '021818.txt': 0.0, '059383.txt': 0.0, '089468.txt': 0.0, '005267.txt': 0.0, '095993.txt': 0.0, '010024.txt': 0.0, '006922.txt': 0.0, '007286.txt': 0.0, '026185.txt': 0.0, '001947.txt': 0.0, '035994.txt': 0.0, '077844.txt': 0.0, '045673.txt': 0.0, '032248.txt': 0.0, '035718.txt': 0.0, '012111.txt': 0.0, '055858.txt': 0.0, '052454.txt': 0.0, '013029.txt': 0.0, '043298.txt': 0.0, '037221.txt': 0.0, '089515.txt': 0.0, '041390.txt': 0.0, '057191.txt': 0.0, '021601.txt': 0.0, '050666.txt': 0.0, '007698.txt': 0.0, '027247.txt': 0.0, '028416.txt': 0.0, '049261.txt': 0.0, '022160.txt': 0.0, '097987.txt': 0.0, '042957.txt': 0.0, '081163.txt': 0.0, '061196.txt': 0.0, '016748.txt': 0.0, '045104.txt': 0.0, '046762.txt': 0.0, '074300.txt': 0.0, '036157.txt': 0.0, '097104.txt': 0.0, '089065.txt': 0.0, '007283.txt': 0.0, '002716.txt': 0.0, '083224.txt': 0.0, '050382.txt': 0.0, '098981.txt': 0.0, '023107.txt': 0.0, '057494.txt': 0.0, '017701.txt': 0.0, '001153.txt': 0.0, '022534.txt': 0.0, '050237.txt': 0.0, '045251.txt': 0.0, '031872.txt': 0.0, '078963.txt': 0.0, '088672.txt': 0.0, '094198.txt': 0.0, '016253.txt': 0.0, '094548.txt': 0.0, '033796.txt': 0.0, '090433.txt': 0.0, '050486.txt': 0.0, '005768.txt': 0.0, '087656.txt': 0.0, '020090.txt': 0.0, '024403.txt': 0.0, '097279.txt': 0.0, '070553.txt': 0.0, '004492.txt': 0.0, '089818.txt': 0.0, '099706.txt': 0.0, '031074.txt': 0.0, '031140.txt': 0.0, '086163.txt': 0.0, '025252.txt': 0.0, '089577.txt': 0.0, '003681.txt': 0.0, '009309.txt': 0.0, '091168.txt': 0.0, '049599.txt': 0.0, '030557.txt': 0.0, '052265.txt': 0.0, '039860.txt': 0.0, '097970.txt': 0.0, '001127.txt': 0.0, '056002.txt': 0.0, '044549.txt': 0.0, '005430.txt': 0.0, '033709.txt': 0.0, '025422.txt': 0.0})\n",
            "0.5\n",
            "candidadates:  {'095905.txt': 0.4016641135842197, '017617.txt': 0.4133299187821542, '041229.txt': 0.41356683337447153, '004851.txt': 0.4138204691074443, '049315.txt': 0.42057051104707155, '040639.txt': 0.4221886953002507, '020730.txt': 0.4222085185126598, '075040.txt': 0.42535583006115585, '069049.txt': 0.45965139058246385, '089987.txt': 0.9453212074282287}\n",
            "gabarito:  ['049315.txt', '004851.txt', '082291.txt', '072495.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjS_BieZRscs"
      },
      "outputs": [],
      "source": [
        "#from nltk.metrics.scores import recall\n",
        "\n",
        "  #Verificação PRECISION\n",
        "#print(results_raw.items())\n",
        "#query = '008447.txt'\n",
        "#print('candidadates: ',results_raw[query])\n",
        "#print('gabarito: ', queries_to_refs[query][::-1])\n",
        "#print(len(results_raw[query]))\n",
        "#print(precision_N[query])\n",
        "#print('precisao: ', 4/34)\n",
        "\n",
        "##Recall\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "#4. Pre-selection based on the topics:\n",
        "#Select only K-Candidate set = docs in the same top K topic as Q\n",
        "-Para doc ID Q\n",
        "  -Olha para as referências de Q\n",
        "    -Se o reference dominant topics is within the document top K topics\n",
        "      -Salva no vetor pre-selection deste doc\n",
        "\n",
        "#5. Select the X most similar docs with Q and K-Candidate set\n",
        "1. Selecionar do docs_embedding apenas os documentos da lista de pre-selection\n",
        "2. Roda o algoritmo de similaridade\n",
        "3. Recupera os X mais similares\n",
        "4. Cria map : doc_id_Q -> doc_id_Answers \n",
        "\n",
        "->Fazer isso para todos os itens do vetor\n",
        "\n",
        "\n",
        "# sm = Similarity()\n",
        "# sm.compute_cosine_similarity(preprocessed_corpus, list_of_ids)\n",
        "\n",
        "#Select top X\n",
        "\n",
        "#embed_query = [Doc2Vec(query)]\n",
        "#embed_candidatos = [[embed_doc1], [embed_doc2], [embed_doc3]]\n",
        "\n",
        "#Calculate the cosine similarity\n",
        "#from sklearn.metrics.pairwise import cosine_similarity\n",
        "#sim = cosine_similarity(X=embed_query, Y=embed_candidatos)\n",
        "\n",
        "#ind = np.argpartition(sim, -20)[-20:]\n",
        "#top = ind[np.argsort(sim[ind])]\n",
        "\n",
        "'''\n",
        "\n",
        "#print(doc_to_dominant_topic)\n",
        "#print(inverted_map)\n",
        "#print(inverted_map[21.0].union(inverted_map[18.0]))\n",
        "# print(len(inverted_map[117.0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save list_of_ids\n",
        "# open file in write mode\n",
        "with open(r'/content/drive/MyDrive/Colab-Notebooks/luisa-novaes/list_of_ids', 'w') as fp:\n",
        "    for id in list_of_ids:\n",
        "        # write each item on a new line\n",
        "        fp.write(\"%s\\n\" % id)\n",
        "    print('Done')\n",
        "\n",
        "print(list_of_ids)"
      ],
      "metadata": {
        "id": "8Om92zLoYhWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxN1n4sGx6x_"
      },
      "outputs": [],
      "source": [
        "#Precision\n",
        "\n",
        "def precision(doc_to_dominant_topic: map, doc_to_refs: map, lista_precision): \n",
        " result = Counter()\n",
        " for (doc, v) in doc_to_refs.items():\n",
        "   for ref in doc_to_refs[doc]:\n",
        "    if (doc_to_dominant_topic[ref] == doc_to_dominant_topic[doc]):\n",
        "     result[doc] = result[doc] + 1\n",
        "   result[doc] = result[doc] / precision_docs[doc_to_dominant_topic[doc]]\n",
        " return result\n",
        "\n",
        "print(precision_docs, '\\n')\n",
        "print(doc_to_dominant_topic)\n",
        "print(precision_docs[doc_to_dominant_topic['021359.txt']])\n",
        "\n",
        "#print(precision_docs, '\\n')\n",
        "#print(doc_to_dominant_topic, '\\n')\n",
        "#print(doc_to_refs['030394.txt'],'\\n')\n",
        "#print(doc_to_refs.items(), '\\n')\n",
        "#print(precision_docs[doc_to_dominant_topic['030394.txt']])\n",
        "\n",
        "def precision(doc_to_dominant_topic: map, doc_to_refs: map, lista_precision): \n",
        " result = Counter()\n",
        " for (doc, v) in doc_to_refs.items():\n",
        "   for ref in doc_to_refs[doc]:\n",
        "    if (doc_to_dominant_topic[ref] == doc_to_dominant_topic[doc]):\n",
        "     result[doc] = result[doc] + 1\n",
        "   result[doc] = result[doc] / precision_docs[doc_to_dominant_topic[doc]]\n",
        " return result\n",
        "\n",
        "# transform doc_to_dominant_topic in df\n",
        "doc_to_dominant_df = pd.DataFrame(doc_to_dominant_topic.items(), columns=['Doc', 'Topic'])\n",
        "# Count how many times each topic is found (most common)\n",
        "series = doc_to_dominant_df['Topic'].value_counts()\n",
        "# Convert to data frame\n",
        "precision_docs = series.to_frame('Total_Docs') \n",
        "# Add index as a new column\n",
        "precision_docs['Topic'] = list(precision_docs.index)\n",
        "# Reset the index\n",
        "precision_docs = precision_docs.sort_values('Topic')\n",
        "precision_docs = precision_docs.reset_index(drop=True)\n",
        "precision_docs = precision_docs.reindex(columns=['Topic','Total_Docs'])\n",
        "#convert to dict\n",
        "precision_docs = dict(precision_docs.values)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}